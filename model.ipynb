{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6cbb66d-c81e-4eec-b18f-67f4a543caf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")  # non-GUI backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "637ccd86-7443-4388-9c59-c57927e14635",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d3feb7-4616-4fab-bcd3-aa8c8a9302b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd0f0d3c-bb85-48dc-8eb4-3b855637c4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 5050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e64ccd6-8982-46c8-8c5e-9eb4e6108c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # Main PyTorch package for tensor operations and deep learning functionalities\n",
    "import torch.nn as nn  # Submodule for defining neural network layers like nn.Linear, nn.Conv2d, etc.\n",
    "import torch.optim as optim  # Optimizers such as Adam, SGD for model training\n",
    "from torch.utils.data import Dataset, DataLoader, random_split  # Dataset: abstract class for data, DataLoader: batching, random_split: dataset partitioning\n",
    "import numpy as np  # Numerical operations and array manipulation, often used for preprocessing and conversion\n",
    "from pymongo import MongoClient  # Interface for connecting to MongoDB — used to retrieve or store PSF data and labels\n",
    "from tqdm import tqdm  # Displays smart progress bars for training/validation loops and batch iterations\n",
    "import copy  # Enables deep copying of model weights (useful for checkpointing the best model)\n",
    "import matplotlib.pyplot as plt  # Plotting utility for visualizations like training/validation loss curves and PSF comparisons\n",
    "import warnings  # Python’s standard warning control module — used here to suppress non-critical warnings\n",
    "import torch.nn.functional as F  # Provides stateless versions of activation functions and loss functions (e.g., F.relu, F.mse_loss)\n",
    "import os  # OS interface to create directories, manage file paths, and access environment-specific variables\n",
    "import yaml  # Parses YAML configuration files into Python dictionaries (used if OmegaConf isn’t applied)\n",
    "from omegaconf import OmegaConf  # More powerful configuration handler than `yaml` — supports dot-access and structured configs\n",
    "from omegaconf import DictConfig, ListConfig  # Typed configuration containers supporting dot access and validation\n",
    "import random  # Python’s built-in random module — used for global seeding and randomness control in splits or shuffling\n",
    "\n",
    "warnings.filterwarnings('ignore')  # Silences all warning messages to keep console/log outputs clean during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a78df27-7e60-44ea-b980-2d72b0f36f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the dataset configuration file exists\n",
    "if not os.path.exists('./dataset_config.yaml'):\n",
    "    raise FileNotFoundError(\"Dataset config file not found.\")\n",
    "\n",
    "# Check if the model configuration file exists\n",
    "if not os.path.exists('./model_config.yaml'):\n",
    "    raise FileNotFoundError(\"Model config file not found.\")\n",
    "\n",
    "# Load dataset configuration from a YAML file into an OmegaConf object\n",
    "dataset_config = OmegaConf.load('./dataset_config.yaml')  # Contains settings like dataset paths, preprocessing, seed, etc.\n",
    "\n",
    "# Load model architecture and training configuration from another YAML file\n",
    "model_config = OmegaConf.load('./model_config.yaml')  # Contains model details like layers, optimizer settings, and training hyperparameters\n",
    "\n",
    "# Merge the dataset and model configs into a single config dictionary-like object\n",
    "config = OmegaConf.merge(dataset_config, model_config)  # Allows unified access to all config parameters using dot notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8aef4362-d560-42ca-a753-6c7aa55cea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate MongoDB configuration\n",
    "if 'mongodb' not in config:\n",
    "    raise KeyError(\"MongoDB configuration section is missing in the configuration file.\")\n",
    "\n",
    "if 'uri' not in config['mongodb']:\n",
    "    raise KeyError(\"MongoDB URI is missing in the configuration file.\")\n",
    "if 'database' not in config['mongodb']:\n",
    "    raise KeyError(\"MongoDB database name is missing in the configuration file.\")\n",
    "if 'collection' not in config['mongodb']:\n",
    "    raise KeyError(\"MongoDB collection name is missing in the configuration file.\")\n",
    "\n",
    "if not isinstance(config['mongodb']['uri'], str):\n",
    "    raise TypeError(\"MongoDB URI must be a string.\")\n",
    "if not isinstance(config['mongodb']['database'], str):\n",
    "    raise TypeError(\"MongoDB database name must be a string.\")\n",
    "if not isinstance(config['mongodb']['collection'], str):\n",
    "    raise TypeError(\"MongoDB collection name must be a string.\")\n",
    "\n",
    "# Attempt to connect to MongoDB and create index\n",
    "try:\n",
    "    client = MongoClient(config['mongodb']['uri'])  # Connect to MongoDB using the URI\n",
    "    db = client[config['mongodb']['database']]  # Access the database\n",
    "    full_collection = db[config['mongodb']['collection']]  # Access the collection\n",
    "    full_collection.create_index(\"index\", name=\"idx\", unique=True)  # Ensure unique index field\n",
    "except Exception as e:\n",
    "    raise ConnectionError(f\"Failed to connect to MongoDB or create index: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4cf6e90-703f-4936-a7bf-db6f97d6e9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate and set seeds for reproducibility\n",
    "if \"dataset\" not in config:\n",
    "    raise KeyError(\"Missing 'dataset' section in the configuration.\")\n",
    "if 'seed' not in config['dataset']:\n",
    "    raise KeyError(\"Seed value is missing from the configuration under 'dataset' section.\")\n",
    "if not isinstance(config['dataset']['seed'], int):\n",
    "    raise TypeError(\"`seed` value must be an integer.\")\n",
    "\n",
    "seed = config[\"dataset\"][\"seed\"]\n",
    "\n",
    "if not isinstance(seed, int):\n",
    "    raise TypeError(f\"Seed must be an integer, got {type(seed)} instead.\")\n",
    "\n",
    "# Set seeds across all relevant libraries\n",
    "random.seed(seed)  # Python's built-in random\n",
    "np.random.seed(seed)  # NumPy random\n",
    "torch.manual_seed(seed)  # PyTorch CPU RNG\n",
    "torch.cuda.manual_seed_all(seed)  # PyTorch CUDA RNGs (all GPUs)\n",
    "torch.backends.cudnn.deterministic = True  # Force deterministic behavior in cuDNN\n",
    "torch.backends.cudnn.benchmark = False  # Disable auto-tuning for reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "047308e2-dd45-4090-8d10-12c8da339b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_mirrors(config):\n",
    "    \"\"\"\n",
    "    Validate the mirror configurations in the config file.\n",
    "\n",
    "    This function checks that all required mirror segments and their parameters are properly defined.\n",
    "    It verifies that each segment has the necessary piston, tip, and tilt parameters and that \n",
    "    they are correctly formatted.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    config : dict\n",
    "        Configuration dictionary loaded from the YAML file\n",
    "        \n",
    "    Raises:\n",
    "    -------\n",
    "    ValueError\n",
    "        If any mirror configuration is invalid or missing\n",
    "    \"\"\"\n",
    "    # Validate primary mirror configuration\n",
    "    if 'primary_mirror' not in config:\n",
    "        raise ValueError(\"Missing 'primary_mirror' section in config.\")\n",
    "\n",
    "    # Check if 'segments' exists in the primary_mirror section\n",
    "    if 'segments' not in config['primary_mirror']:\n",
    "        raise ValueError(\"Missing 'segments' key in 'primary_mirror' section.\")\n",
    "\n",
    "    segments = config['primary_mirror']['segments']\n",
    "    # Use isinstance with (list, ListConfig) to handle both Python lists and OmegaConf ListConfig objects\n",
    "    if not isinstance(segments, (list, ListConfig)):\n",
    "        raise ValueError(\"'segments' under 'primary_mirror' must be a list.\")\n",
    "\n",
    "    found_ids = set()\n",
    "    missing_params_by_id = {}\n",
    "\n",
    "    # Validate each segment in the primary mirror\n",
    "    for i, segment in enumerate(segments):\n",
    "        # Validate segment has an ID\n",
    "        if 'id' not in segment:\n",
    "            raise ValueError(f\"Segment at index {i} is missing 'id' key.\")\n",
    "        \n",
    "        seg_id = segment['id']\n",
    "        if not isinstance(seg_id, int):\n",
    "            raise ValueError(f\"Segment at index {i} has a non-integer 'id': {seg_id}\")\n",
    "        \n",
    "        # Check for duplicate IDs\n",
    "        if seg_id in found_ids:\n",
    "            raise ValueError(f\"Duplicate segment ID found: {seg_id}\")\n",
    "        \n",
    "        found_ids.add(seg_id)\n",
    "\n",
    "        # Check for required parameters (piston, tip, tilt)\n",
    "        missing_keys = []\n",
    "        for key in ['piston', 'tip', 'tilt']:\n",
    "            if key not in segment:\n",
    "                missing_keys.append(key)\n",
    "                continue\n",
    "\n",
    "            # Validate parameter format (must be a dict with either 'static', 'range', or 'untrained_range')\n",
    "            param_dict = segment[key]\n",
    "            if not isinstance(param_dict, (dict, DictConfig)):\n",
    "                raise ValueError(f\"{key} for segment {seg_id} must be a dictionary.\")\n",
    "            \n",
    "            # Check that exactly one of the allowed keys is present\n",
    "            allowed_keys = {'static', 'range', 'untrained_range'}\n",
    "            param_keys = set(param_dict.keys())\n",
    "            if not param_keys.issubset(allowed_keys):\n",
    "                invalid_keys = param_keys - allowed_keys\n",
    "                raise ValueError(f\"{key} for segment {seg_id} contains invalid keys: {invalid_keys}. \"\n",
    "                               f\"Must use only: {allowed_keys}.\")\n",
    "            \n",
    "            if len(param_dict) != 1:\n",
    "                raise ValueError(f\"{key} for segment {seg_id} must contain exactly one of {allowed_keys}.\")\n",
    "            \n",
    "            # Get the parameter type (static or range)\n",
    "            inner_key = next(iter(param_dict))\n",
    "            \n",
    "            # Validate the value based on parameter type\n",
    "            value = param_dict[inner_key]\n",
    "            if inner_key == 'static':\n",
    "                # Static values must be floats\n",
    "                if not isinstance(value, (int, float)):\n",
    "                    raise ValueError(f\"{key} for segment {seg_id} must be a number under 'static', but got {type(value).__name__}.\")\n",
    "            else:\n",
    "                # Range values must be a list of exactly 2 floats\n",
    "                if not isinstance(value, (list, ListConfig)):\n",
    "                    raise ValueError(f\"{key} for segment {seg_id} under '{inner_key}' must be a list, but got {type(value).__name__}.\")\n",
    "                \n",
    "                if len(value) != 2:\n",
    "                    raise ValueError(f\"{key} for segment {seg_id} under '{inner_key}' must be a list of exactly 2 values.\")\n",
    "                \n",
    "                if not all(isinstance(v, (int, float)) for v in value):\n",
    "                    non_numeric = [i for i, v in enumerate(value) if not isinstance(v, (int, float))]\n",
    "                    raise ValueError(f\"{key} for segment {seg_id} under '{inner_key}' contains non-numeric values at positions {non_numeric}.\")\n",
    "\n",
    "        # Track any missing parameters for this segment\n",
    "        if missing_keys:\n",
    "            missing_params_by_id[seg_id] = missing_keys\n",
    "\n",
    "    # Ensure all required segments (1-6) are present\n",
    "    required_ids = set(range(1, 7))\n",
    "    missing_ids = sorted(list(required_ids - found_ids))\n",
    "    if missing_ids:\n",
    "        raise ValueError(f\"Missing segments with IDs: {missing_ids}\")\n",
    "\n",
    "    # Report any segments with missing parameters\n",
    "    if missing_params_by_id:\n",
    "        msg = \"Some segments are missing required parameters:\\n\"\n",
    "        for sid, keys in missing_params_by_id.items():\n",
    "            msg += f\"  Segment ID {sid}: missing {', '.join(keys)}\\n\"\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    # Validate secondary mirror configuration\n",
    "    if 'secondary_mirror' not in config:\n",
    "        raise ValueError(\"Missing 'secondary_mirror' section in config.\")\n",
    "\n",
    "    secondary_params = config['secondary_mirror']\n",
    "\n",
    "    # Check that secondary mirror has all required parameters\n",
    "    for param in ['piston', 'tip', 'tilt']:\n",
    "        if param not in secondary_params:\n",
    "            raise ValueError(f\"Missing '{param}' parameter in 'secondary_mirror'.\")\n",
    "\n",
    "        # Validate parameter format\n",
    "        param_dict = secondary_params[param]\n",
    "        if not isinstance(param_dict, (dict, DictConfig)):\n",
    "            raise ValueError(f\"{param} for secondary mirror must be a dictionary, but got {type(param_dict).__name__}.\")\n",
    "        \n",
    "        # Check that exactly one of the allowed keys is present\n",
    "        allowed_keys = {'static', 'range', 'untrained_range'}\n",
    "        param_keys = set(param_dict.keys())\n",
    "        if not param_keys.issubset(allowed_keys):\n",
    "            invalid_keys = param_keys - allowed_keys\n",
    "            raise ValueError(f\"{param} for secondary mirror contains invalid keys: {invalid_keys}. \"\n",
    "                           f\"Must use only: {allowed_keys}.\")\n",
    "        \n",
    "        if len(param_dict) != 1:\n",
    "            raise ValueError(f\"{param} for secondary mirror must contain exactly one of {allowed_keys}.\")\n",
    "        \n",
    "        # Get the parameter type\n",
    "        inner_key = next(iter(param_dict))\n",
    "        \n",
    "        # Validate the value based on parameter type\n",
    "        value = param_dict[inner_key]\n",
    "        if inner_key == 'static':\n",
    "            # Static values must be numbers (int or float)\n",
    "            if not isinstance(value, (int, float)):\n",
    "                raise ValueError(f\"{param} for secondary mirror must be a number under 'static', but got {type(value).__name__}.\")\n",
    "        else:\n",
    "            # Range values must be a list of two numbers\n",
    "            if not isinstance(value, (list, ListConfig)):\n",
    "                raise ValueError(f\"{param} for secondary mirror under '{inner_key}' must be a list, but got {type(value).__name__}.\")\n",
    "            \n",
    "            if len(value) != 2:\n",
    "                raise ValueError(f\"{param} for secondary mirror under '{inner_key}' must be a list of exactly 2 values.\")\n",
    "            \n",
    "            if not all(isinstance(v, (int, float)) for v in value):\n",
    "                non_numeric = [i for i, v in enumerate(value) if not isinstance(v, (int, float))]\n",
    "                raise ValueError(f\"{param} for secondary mirror under '{inner_key}' contains non-numeric values at positions {non_numeric}.\")\n",
    "\n",
    "    return True  # Return True if validation passes\n",
    "\n",
    "validate_mirrors(config);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89ec6e6e-c3a5-458d-b960-440a9a8b24e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_parameters(config):\n",
    "    # Lists to categorize parameters based on training roles\n",
    "    to_predict = []           # Parameters with defined \"range\" — included in training\n",
    "    untrained_predict = []    # Parameters with \"untrained_range\" — excluded from training, used for testing/generalization\n",
    "    not_to_predict = []       # Parameters marked \"static\" — constant values, not predicted\n",
    "\n",
    "    # Iterate through each segment in the primary mirror\n",
    "    for segment in config[\"primary_mirror\"][\"segments\"]:\n",
    "        seg_id = segment[\"id\"]  # Segment identifier (e.g., 1, 2, 3...)\n",
    "\n",
    "        for param in [\"piston\", \"tip\", \"tilt\"]:  # Loop through each degree of freedom (DoF)\n",
    "            param_config = segment[param]  # Access configuration for this DoF\n",
    "            param_name = f\"primary_{seg_id}_{param}\"  # Create parameter name, e.g., \"primary_1_piston\"\n",
    "\n",
    "            # Classify the parameter into the appropriate category\n",
    "            if \"range\" in param_config:\n",
    "                to_predict.append(param_name)  # Included in model training\n",
    "            elif \"untrained_range\" in param_config:\n",
    "                untrained_predict.append(param_name)  # Held out for generalization testing\n",
    "            elif \"static\" in param_config:\n",
    "                not_to_predict.append(param_name)  # Fixed parameter, excluded from prediction\n",
    "\n",
    "    # Process secondary mirror parameters in the same way\n",
    "    for param in [\"piston\", \"tip\", \"tilt\"]:\n",
    "        param_config = config[\"secondary_mirror\"][param]  # Access config for each DoF\n",
    "        param_name = f\"secondary_{param}\"  # e.g., \"secondary_tip\"\n",
    "\n",
    "        # Classify the secondary mirror parameter\n",
    "        if \"range\" in param_config:\n",
    "            to_predict.append(param_name)\n",
    "        elif \"untrained_range\" in param_config:\n",
    "            untrained_predict.append(param_name)\n",
    "        elif \"static\" in param_config:\n",
    "            not_to_predict.append(param_name)\n",
    "\n",
    "    # Return all categorized parameter names\n",
    "    return to_predict, untrained_predict, not_to_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9970fc2c-0ffd-42b9-a7c6-54b0700abddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['secondary_piston']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_predict, untrained_predict, not_to_predict = extract_parameters(config)\n",
    "to_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0434e30-8fa7-4074-bea3-726897f27daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to flatten a nested dictionary into a flat dictionary\n",
    "def flatten_dict(d, parent_key='', sep='_'):\n",
    "    items = []  # Initialize an empty list to store the flattened key-value pairs\n",
    "    \n",
    "    for k, v in d.items():  # Iterate through each key-value pair in the dictionary\n",
    "        # If a parent_key exists, join it with the current key using the separator\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k  # Construct the flattened key\n",
    "        \n",
    "        if isinstance(v, dict):  # Check if the value is a dictionary\n",
    "            # Recursively flatten the nested dictionary and extend the result to the items list\n",
    "            items.extend(flatten_dict(v, new_key, sep=sep).items())  # Flatten the nested dict and add to the list\n",
    "        else:\n",
    "            # If the value is not a dictionary, add the key-value pair directly to the list\n",
    "            items.append((new_key, v))  # Add the key-value pair to the flattened list\n",
    "    \n",
    "    return dict(items)  # Return the flattened dictionary as a dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8ebb036-3eeb-4c7b-8e71-dc002c2f41dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBImageDataset(Dataset):\n",
    "    def __init__(self, config, shuffle=True):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with merged config and shuffle flag.\n",
    "\n",
    "        :param config: Merged configuration object\n",
    "        :param shuffle: Flag to control shuffling of dataset\n",
    "        \"\"\"\n",
    "        self.config = config  # Store the dataset configuration\n",
    "        self.mongo_collection = full_collection  # MongoDB collection for fetching documents (must be pre-initialized)\n",
    "        self.target_keys = to_predict  # Target parameters that the model should predict\n",
    "        self.shuffle = shuffle  # Flag for whether to shuffle the dataset on initialization\n",
    "\n",
    "        # --- Defocus Validation and Handling ---\n",
    "        raw_defocus = self.config[\"dataset\"][\"use_defocus\"]  # Raw value from config (can be int or float)\n",
    "\n",
    "        # Ensure that 'use_defocus' is a numeric type (int or float)\n",
    "        if not isinstance(raw_defocus, (int, float)):\n",
    "            raise TypeError(f\"'use_defocus' must be an int or float, got {type(raw_defocus)}.\")\n",
    "        \n",
    "        # Convert to boolean: 0 means no defocus used, non-zero means defocus applied\n",
    "        self.use_defocused = bool(raw_defocus)\n",
    "\n",
    "        # --- Transform Validation ---\n",
    "        self.transform = self.config[\"dataset\"][\"transform\"]  # Type of transform to apply on PSFs\n",
    "        allowed_transforms = {\"linear\", \"sqrt\", \"log\"}  # Only these three are allowed\n",
    "        \n",
    "        # Validate that transform type is one of the supported options\n",
    "        if self.transform not in allowed_transforms:\n",
    "            raise ValueError(f\"Unsupported transform '{self.transform}'. Must be one of {allowed_transforms}.\")\n",
    "\n",
    "        # --- Load Dataset Indexes ---\n",
    "        # Fetch all document indexes from the MongoDB collection (used to retrieve samples efficiently)\n",
    "        self.doc_indexes = list(\n",
    "            self.mongo_collection.find({}, {\"index\": 1, \"_id\": 0}).sort(\"index\", 1)\n",
    "        )\n",
    "        \n",
    "        # Raise an error if no data was found\n",
    "        if not self.doc_indexes:\n",
    "            raise ValueError(\"No documents found in the collection. Ensure each document has an 'index' field.\")\n",
    "\n",
    "        self.total_docs = len(self.doc_indexes)  # Store the number of total samples available\n",
    "\n",
    "        # Shuffle the list of indexes if requested\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.doc_indexes)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Returns the total number of samples in the dataset\n",
    "        return self.total_docs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Fetch and process a data item by its index.\n",
    "\n",
    "        :param idx: Index of the sample\n",
    "        :return: A tuple of image tensor, (optional magnitude tensor), and target tensor\n",
    "        \"\"\"\n",
    "        # Get the unique document index for this sample\n",
    "        unique_index = self.doc_indexes[idx][\"index\"]\n",
    "        \n",
    "        # Retrieve the full document using its unique index\n",
    "        doc = self.mongo_collection.find_one({\"index\": unique_index})\n",
    "        \n",
    "        # Flatten the document into a single-level dictionary for easier access\n",
    "        flat = flatten_dict(doc)\n",
    "\n",
    "        # --- Extract Target Parameters ---\n",
    "        targets = [flat[key] for key in self.target_keys]  # Extract target values from the flattened dict\n",
    "        target_tensor = torch.tensor(targets, dtype=torch.float32)  # Convert to PyTorch tensor\n",
    "\n",
    "        # --- Load PSF(s) ---\n",
    "        nominal = np.array(doc[\"psf_nominal_array\"], dtype=np.float32)  # Load nominal PSF\n",
    "\n",
    "        # Optionally load defocused PSF if defocus is enabled\n",
    "        if self.use_defocused:\n",
    "            defocused = np.array(doc[\"psf_defocused_array\"], dtype=np.float32)\n",
    "\n",
    "        # --- Apply Transformation ---\n",
    "        if self.transform == \"sqrt\":\n",
    "            nominal = np.sqrt(nominal)  # Apply square root transform\n",
    "            if self.use_defocused:\n",
    "                defocused = np.sqrt(defocused)\n",
    "        elif self.transform == \"log\":\n",
    "            nominal = np.log(nominal + 1e-6)  # Apply log transform (with epsilon for stability)\n",
    "            if self.use_defocused:\n",
    "                defocused = np.log(defocused + 1e-6)\n",
    "        # No transform applied for \"linear\"\n",
    "\n",
    "        # --- Stack PSFs ---\n",
    "        image_channels = [nominal]  # Start with nominal PSF as the first channel\n",
    "        if self.use_defocused:\n",
    "            image_channels.append(defocused)  # Add defocused PSF if applicable\n",
    "\n",
    "        image_np = np.stack(image_channels, axis=0)  # Stack along channel axis (C, H, W)\n",
    "        image_tensor = torch.from_numpy(image_np)  # Convert to PyTorch tensor\n",
    "\n",
    "        # --- Optional Phase-Magnitude Features ---\n",
    "        if self.config[\"dataset\"][\"use_phase_mag\"]:\n",
    "            row_col_features = []  # Store 1D features from frequency domain\n",
    "            psfs = [nominal]  # Start with nominal PSF\n",
    "            if self.use_defocused:\n",
    "                psfs.append(defocused)  # Include defocused if applicable\n",
    "\n",
    "            for img in psfs:\n",
    "                fft_result = np.fft.fft2(img)  # Perform inverse FFT\n",
    "                fft_shifted_result = np.fft.fftshift(fft_result)\n",
    "                magnitude = np.abs(fft_shifted_result)  # Get magnitude spectrum\n",
    "                h, w = magnitude.shape\n",
    "                mid_row = magnitude[h // 2, :]  # Extract middle row\n",
    "                mid_row_norm = (mid_row - np.min(mid_row))/(np.max(mid_row) - np.min(mid_row))\n",
    "                mid_col = magnitude[:, w // 2]  # Extract middle column\n",
    "                mid_col_norm = (mid_col - np.min(mid_col))/(np.max(mid_col) - np.min(mid_col))\n",
    "                row_col_features.extend([mid_row_norm, mid_col_norm])  # Append both\n",
    "\n",
    "            # Concatenate and convert to tensor\n",
    "            magnitude_tensors = torch.tensor(np.concatenate(row_col_features), dtype=torch.float32)\n",
    "            return image_tensor, magnitude_tensors, target_tensor  # Return all three tensors\n",
    "\n",
    "        # Return image and target tensors (no magnitude features)\n",
    "        return image_tensor, target_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89cf0367-aac3-4b60-8a0b-6d69a9603e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Validate Config Values ---\n",
    "\n",
    "# Validate that 'seed' is present and is an integer\n",
    "if \"seed\" not in config[\"dataset\"]:\n",
    "    raise KeyError(\"'seed' must be defined in config['dataset'].\")\n",
    "if not isinstance(config[\"dataset\"][\"seed\"], int):\n",
    "    raise TypeError(f\"'seed' must be an integer, got {type(config['dataset']['seed'])}.\")\n",
    "\n",
    "# Validate that 'batch_size' is present and is an integer\n",
    "if \"batch_size\" not in config[\"training\"]:\n",
    "    raise KeyError(\"'batch_size' must be defined in config['training'].\")\n",
    "if not isinstance(config[\"training\"][\"batch_size\"], int):\n",
    "    raise TypeError(f\"'batch_size' must be an integer, got {type(config['training']['batch_size'])}.\")\n",
    "\n",
    "# --- Load the Full Dataset ---\n",
    "\n",
    "# Load the full dataset from a single collection\n",
    "full_dataset = DBImageDataset(\n",
    "    config=config,  # Pass the configuration containing all dataset-related settings\n",
    "    shuffle=False   # Disable shuffling here; shuffling will be handled at DataLoader level or after splitting\n",
    ")\n",
    "\n",
    "# --- Dataset Splitting ---\n",
    "\n",
    "# Define split ratios for the dataset\n",
    "train_ratio = 0.8  # 80% for training\n",
    "val_ratio = 0.1    # 10% for validation\n",
    "test_ratio = 0.1   # 10% for testing\n",
    "\n",
    "# Calculate total number of samples\n",
    "total_size = len(full_dataset)\n",
    "\n",
    "# Compute sizes for each split\n",
    "train_size = int(train_ratio * total_size)\n",
    "val_size = int(val_ratio * total_size)\n",
    "test_size = total_size - train_size - val_size  # Ensure all samples are used\n",
    "\n",
    "# Extract and use the seed from config\n",
    "split_seed = config[\"dataset\"][\"seed\"]\n",
    "\n",
    "# Perform reproducible random split using the validated seed\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    full_dataset,\n",
    "    [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(split_seed)\n",
    ")\n",
    "\n",
    "# --- Create DataLoaders ---\n",
    "\n",
    "# Extract validated batch size\n",
    "batch_size = config[\"training\"][\"batch_size\"]\n",
    "\n",
    "# Create DataLoader for each dataset split\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)   # Shuffle training data\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)      # No shuffle for validation\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)    # No shuffle for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cc80a80-0c33-47d5-aa28-d66109bc30f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetRegression(nn.Module):\n",
    "    def __init__(self, config, num_outputs):\n",
    "        super(ResNetRegression, self).__init__()\n",
    "        self.cfg = config[\"model\"]\n",
    "        self.use_phase_mag = config[\"dataset\"][\"use_phase_mag\"]  # Flag to use phase-magnitude ANN branch\n",
    "        self.merge_strategy = self.cfg[\"add_magnitude_ann\"][\"merge_strategy\"] if self.use_phase_mag else None\n",
    "\n",
    "        # --- Input Configuration ---\n",
    "        zemax_sampling = config[\"zemax\"][\"sampling\"]\n",
    "        self.image_size = 64 * 2**(zemax_sampling-1)  # Assumes 64x64 base PSF size scaled by sampling\n",
    "        self.input_channels = 2 if config[\"dataset\"][\"use_defocus\"] else 1  # 1 for nominal, 2 if defocus is used\n",
    "        self.output_dim = num_outputs  # Number of output regression parameters\n",
    "\n",
    "        # --- Convolutional Backbone ---\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        in_channels = self.input_channels\n",
    "        for layer_cfg in self.cfg[\"conv_layers\"]:\n",
    "            out_channels = layer_cfg[\"out_channels\"]\n",
    "            kernel_size = layer_cfg[\"kernel_size\"]\n",
    "            padding = kernel_size // 2  # Keep spatial dimensions consistent\n",
    "            activation = layer_cfg.get(\"activation\", True)  # Default to using ReLU\n",
    "\n",
    "            block = [nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding, bias=False)]\n",
    "            if activation:\n",
    "                block.append(nn.ReLU())\n",
    "            if layer_cfg[\"pooling\"]:\n",
    "                block.append(nn.MaxPool2d(kernel_size=2, stride=2))  # Downsample spatial size by 2\n",
    "\n",
    "            self.conv_layers.append(nn.Sequential(*block))\n",
    "            in_channels = out_channels  # Update for next layer\n",
    "\n",
    "        # --- Skip Connections ---\n",
    "        self.skip_connections = self.cfg[\"skip_connections\"]\n",
    "        self.skip_ops = nn.ModuleDict()\n",
    "        for skip in self.skip_connections:\n",
    "            from_layer = skip[\"from_layer\"]\n",
    "            to_layer = skip[\"to_layer\"]\n",
    "            skip_type = skip[\"type\"]\n",
    "            in_ch = self.cfg[\"conv_layers\"][from_layer][\"out_channels\"]\n",
    "            out_ch = self.cfg[\"conv_layers\"][to_layer][\"out_channels\"]\n",
    "\n",
    "            if skip_type == \"conv+pool\":\n",
    "                # Align dimensions using downsampling and 1x1 convolution\n",
    "                self.skip_ops[f\"{from_layer}->{to_layer}\"] = nn.Sequential(\n",
    "                    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                    nn.Conv2d(in_ch, out_ch, kernel_size=1, bias=False)\n",
    "                )\n",
    "            elif skip_type == \"add\":\n",
    "                self.skip_ops[f\"{from_layer}->{to_layer}\"] = nn.Identity()\n",
    "\n",
    "        # --- Flatten Layer Output ---\n",
    "        pool_count = sum(1 for l in self.cfg[\"conv_layers\"] if l[\"pooling\"])  # Count how many times we halved the size\n",
    "        spatial_dim = self.image_size // (2 ** pool_count)  # Final spatial dimension after pooling\n",
    "        final_out_channels = self.cfg[\"conv_layers\"][-1][\"out_channels\"]\n",
    "        flatten_dim = final_out_channels * spatial_dim * spatial_dim\n",
    "\n",
    "        # --- Fully Connected Layers for CNN Path ---\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        in_dim = flatten_dim\n",
    "        for dim in self.cfg[\"fc_layers\"]:\n",
    "            self.fc_layers.append(nn.Linear(in_dim, dim))\n",
    "            in_dim = dim\n",
    "        self.final_cnn_out_dim = in_dim  # Save final output size of CNN for merging\n",
    "\n",
    "        # --- ANN Branch for Phase Magnitude Features ---\n",
    "        if self.use_phase_mag:\n",
    "            ann_hidden = self.cfg[\"add_magnitude_ann\"][\"hidden_layers\"]\n",
    "            self.ann_layers = nn.ModuleList()\n",
    "            \n",
    "            # Each PSF contributes row and column, so 2 * image_size; defocused adds another 2\n",
    "            ann_in_dim = self.image_size * 2\n",
    "            if config[\"dataset\"][\"use_defocus\"]:\n",
    "                ann_in_dim *= 2\n",
    "\n",
    "            # Create fully connected layers for ANN\n",
    "            for dim in ann_hidden:\n",
    "                self.ann_layers.append(nn.Linear(ann_in_dim, dim))\n",
    "                ann_in_dim = dim\n",
    "            self.ann_out_dim = ann_in_dim  # Final ANN output dimension\n",
    "\n",
    "            # Determine final dimension after merging CNN and ANN\n",
    "            if self.merge_strategy == \"concat\":\n",
    "                merged_dim = self.final_cnn_out_dim + self.ann_out_dim\n",
    "            elif self.merge_strategy == \"add\":\n",
    "                assert self.final_cnn_out_dim == self.ann_out_dim, \"Merge='add' requires ANN and CNN dims to match\"\n",
    "                merged_dim = self.final_cnn_out_dim\n",
    "\n",
    "            self.fc_out = nn.Linear(merged_dim, self.output_dim)\n",
    "        else:\n",
    "            # If ANN is not used, project CNN output directly\n",
    "            self.fc_out = nn.Linear(self.final_cnn_out_dim, self.output_dim)\n",
    "\n",
    "    def forward(self, *inputs):\n",
    "        # Support for optional second input (magnitude features)\n",
    "        if self.use_phase_mag:\n",
    "            x, magnitude_tensors = inputs\n",
    "        else:\n",
    "            x = inputs[0]\n",
    "\n",
    "        # --- Forward Pass through Conv Layers with Skip Connections ---\n",
    "        outputs = []\n",
    "        for i, layer in enumerate(self.conv_layers):\n",
    "            x = layer(x)\n",
    "            outputs.append(x)\n",
    "\n",
    "            # Apply skip connection if defined for this layer\n",
    "            for skip in self.skip_connections:\n",
    "                if skip[\"to_layer\"] == i:\n",
    "                    from_layer = skip[\"from_layer\"]\n",
    "                    key = f\"{from_layer}->{i}\"\n",
    "                    skip_out = self.skip_ops[key](outputs[from_layer])\n",
    "                    x = x + skip_out  # Residual addition\n",
    "                    outputs[i] = x\n",
    "\n",
    "        x = F.relu(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten tensor for FC layers\n",
    "\n",
    "        # Pass through CNN's FC layers\n",
    "        for fc in self.fc_layers:\n",
    "            x = F.relu(fc(x))\n",
    "\n",
    "        # --- Optional ANN Forward Pass and Merge ---\n",
    "        if self.use_phase_mag:\n",
    "            ann = magnitude_tensors.view(magnitude_tensors.size(0), -1)\n",
    "            for layer in self.ann_layers:\n",
    "                ann = F.relu(layer(ann))\n",
    "\n",
    "            if self.merge_strategy == \"concat\":\n",
    "                x = torch.cat([x, ann], dim=1)\n",
    "            elif self.merge_strategy == \"add\":\n",
    "                x = x + ann\n",
    "\n",
    "        # Final regression output\n",
    "        return self.fc_out(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "215c5290-9983-4a88-b6c2-c3b140afcf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomRMSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        # predictions: Tensor of shape (B, N), where B = batch size, N = number of outputs\n",
    "        # targets:     Tensor of shape (B, N)\n",
    "\n",
    "        # Compute MSE per sample (along the output dimension)\n",
    "        row_mse = torch.mean((predictions - targets) ** 2, dim=1)  # Shape: (B,)\n",
    "\n",
    "        # Take sqrt to get RMSE per sample\n",
    "        row_rmse = torch.sqrt(row_mse)  # Shape: (B,)\n",
    "\n",
    "        # Average over batch to get final scalar loss\n",
    "        return torch.mean(row_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01e9aff9-901b-4136-baf0-dc322300988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(train_losses, val_losses, test_loss, num_epochs, config):\n",
    "    # Extract model name and base path from config\n",
    "    model_name = config[\"model\"][\"name\"]\n",
    "    model_base_path = config[\"model\"][\"base_path\"]\n",
    "    \n",
    "    # Construct the full path where the plot will be saved\n",
    "    save_path = os.path.join(model_base_path, f\"loss_curve_{model_name}.png\")\n",
    "    \n",
    "    # Define epoch indices for the x-axis\n",
    "    epochs = range(1, num_epochs + 1)\n",
    "\n",
    "    # Create a new figure for the loss curves\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Plot training loss (solid line with circles)\n",
    "    plt.plot(epochs, train_losses, label=\"Training Loss\", marker='o', linestyle='-')\n",
    "    \n",
    "    # Plot validation loss (dashed line with squares)\n",
    "    plt.plot(epochs, val_losses, label=\"Validation Loss\", marker='s', linestyle='--')\n",
    "    \n",
    "    # Plot a horizontal line for the test loss\n",
    "    plt.axhline(y=test_loss, color='r', linestyle='-.', label=f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    # Set axis labels and plot title\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Training, Validation & Test Loss — {model_name}\")\n",
    "    \n",
    "    # Show legend and grid for better readability\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Make sure the output directory exists before saving the plot\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    \n",
    "    # Save the plot to the specified file path\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"Training plot saved to: {save_path}\")\n",
    "\n",
    "    # Display the plot inline\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49b1e434-0994-4231-bfdf-d3b8bb2e3f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_loader, val_loader, test_loader, num_epochs, optimizer, criterion, device, config):\n",
    "    \"\"\"\n",
    "    Train and evaluate the model across multiple epochs, including saving the best model based on validation loss.\n",
    "    \n",
    "    :param model: The neural network model to be trained.\n",
    "    :param train_loader: DataLoader for the training dataset.\n",
    "    :param val_loader: DataLoader for the validation dataset.\n",
    "    :param test_loader: DataLoader for the test dataset.\n",
    "    :param num_epochs: Number of epochs to train the model.\n",
    "    :param optimizer: The optimizer used to update model weights.\n",
    "    :param criterion: The loss function used to calculate the loss.\n",
    "    :param device: The device to run the model (e.g., \"cuda\" or \"cpu\").\n",
    "    :param config: Configuration dictionary containing various parameters (e.g., dataset settings, model settings).\n",
    "    \n",
    "    :return: The trained model with the best validation performance.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize variables to track the best validation loss and model weights\n",
    "    best_val_loss = float('inf')  # Start with a high initial value for validation loss\n",
    "    best_model_wts = None  # Placeholder for storing the best model weights\n",
    "\n",
    "    # Lists to track training and validation losses, as well as average losses\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    avg_losses = []\n",
    "\n",
    "    # Retrieve model name, save path, and dataset-specific settings from the config\n",
    "    model_name = config[\"model\"][\"name\"]\n",
    "    model_base_path = config[\"model\"][\"base_path\"]\n",
    "    use_phase_mag = config[\"dataset\"][\"use_phase_mag\"]\n",
    "\n",
    "    # Create the model save directory if it doesn't exist\n",
    "    os.makedirs(model_base_path, exist_ok=True)\n",
    "\n",
    "    # Training loop across all epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        train_loss = 0.0  # Initialize the total training loss for this epoch\n",
    "        train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\", leave=True)  # Progress bar\n",
    "\n",
    "        # Iterate over the batches in the training set\n",
    "        for batch in train_bar:\n",
    "            if use_phase_mag:\n",
    "                # If phase magnitude is used, unpack both image and magnitude features\n",
    "                inputs, magnitude_features, targets = batch\n",
    "                magnitude_features = magnitude_features.to(device)  # Move magnitude features to device\n",
    "                model_input = (inputs.to(device), magnitude_features)  # Create input tuple for the model\n",
    "            else:\n",
    "                # Otherwise, just unpack the image and target\n",
    "                inputs, targets = batch\n",
    "                model_input = (inputs.to(device),)  # Input tuple only contains the image\n",
    "\n",
    "            targets = targets.to(device)  # Move targets to device\n",
    "\n",
    "            optimizer.zero_grad()  # Zero out the gradients from the previous step\n",
    "            outputs = model(*model_input)  # Forward pass through the model\n",
    "            loss = criterion(outputs, targets)  # Compute the loss\n",
    "            loss.backward()  # Backpropagate the loss to compute gradients\n",
    "            optimizer.step()  # Update the model weights using the optimizer\n",
    "\n",
    "            # Accumulate the loss for the entire batch\n",
    "            train_loss += loss.item() * inputs.size(0)  # Multiply by batch size for proper loss averaging\n",
    "            train_bar.set_postfix(loss=f\"{loss.item():.4f}\")  # Update progress bar with current loss\n",
    "\n",
    "        # Calculate the average training loss for this epoch\n",
    "        epoch_train_loss = train_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_train_loss)  # Store the training loss for plotting\n",
    "\n",
    "        # Validation phase (no gradient computation)\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        val_loss = 0.0  # Initialize the validation loss\n",
    "        with torch.no_grad():  # No need to track gradients during validation\n",
    "            val_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Validation\", leave=True)  # Progress bar\n",
    "            # Iterate over validation batches\n",
    "            for batch in val_bar:\n",
    "                if use_phase_mag:\n",
    "                    # Unpack inputs, magnitude features, and targets if using phase magnitude\n",
    "                    inputs, magnitude_features, targets = batch\n",
    "                    magnitude_features = magnitude_features.to(device)\n",
    "                    model_input = (inputs.to(device), magnitude_features)\n",
    "                else:\n",
    "                    # Otherwise, unpack image and targets\n",
    "                    inputs, targets = batch\n",
    "                    model_input = (inputs.to(device),)\n",
    "\n",
    "                targets = targets.to(device)  # Move targets to device\n",
    "\n",
    "                # Forward pass without gradient computation\n",
    "                outputs = model(*model_input)\n",
    "                loss = criterion(outputs, targets)  # Compute the validation loss\n",
    "                val_loss += loss.item() * inputs.size(0)  # Accumulate the loss for the batch\n",
    "                val_bar.set_postfix(loss=f\"{loss.item():.4f}\")  # Update progress bar with current loss\n",
    "\n",
    "        # Calculate the average validation loss for this epoch\n",
    "        epoch_val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(epoch_val_loss)  # Store the validation loss for plotting\n",
    "\n",
    "        # Calculate average loss for this epoch\n",
    "        epoch_avg_loss = (epoch_train_loss + epoch_val_loss) / 2\n",
    "        avg_losses.append(epoch_avg_loss)  # Store average loss for plotting\n",
    "\n",
    "        # Print summary for this epoch\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs} Summary:\")\n",
    "        print(f\"Training Loss: {epoch_train_loss:.4f}\")\n",
    "        print(f\"Validation Loss: {epoch_val_loss:.4f}\")\n",
    "        print(f\"Average Loss: {epoch_avg_loss:.4f}\\n\")\n",
    "\n",
    "        # Save the model with the best validation loss\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss  # Update the best validation loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())  # Save model weights\n",
    "            model_save_path = os.path.join(model_base_path, f\"{model_name}.pth\")  # Define save path\n",
    "            torch.save(best_model_wts, model_save_path)  # Save the model weights to file\n",
    "            print(f\"Saved new best model with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    # Load the best model (with the lowest validation loss)\n",
    "    model.load_state_dict(torch.load(os.path.join(model_base_path, f\"{model_name}.pth\")))\n",
    "\n",
    "    # Test phase: Evaluate the model on the test set\n",
    "    test_loss = 0.0\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():  # No gradients needed during testing\n",
    "        test_bar = tqdm(test_loader, desc=\"Testing Best Model\", leave=True)  # Progress bar\n",
    "        for batch in test_bar:\n",
    "            if use_phase_mag:\n",
    "                # Unpack inputs, magnitude features, and targets if using phase magnitude\n",
    "                inputs, magnitude_features, targets = batch\n",
    "                magnitude_features = magnitude_features.to(device)\n",
    "                model_input = (inputs.to(device), magnitude_features)\n",
    "            else:\n",
    "                # Otherwise, just unpack image and targets\n",
    "                inputs, targets = batch\n",
    "                model_input = (inputs.to(device),)\n",
    "\n",
    "            targets = targets.to(device)  # Move targets to device\n",
    "\n",
    "            # Forward pass for testing\n",
    "            outputs = model(*model_input)\n",
    "            loss = criterion(outputs, targets)  # Compute the test loss\n",
    "            test_loss += loss.item() * inputs.size(0)  # Accumulate the loss for the batch\n",
    "            test_bar.set_postfix(loss=f\"{loss.item():.4f}\")  # Update progress bar with current loss\n",
    "\n",
    "    # Calculate the final test loss\n",
    "    final_test_loss = test_loss / len(test_loader.dataset)\n",
    "    print(f\"\\nFinal Test Loss: {final_test_loss:.4f}\")  # Print the final test loss\n",
    "\n",
    "    # Plot the training history (training/validation loss over epochs)\n",
    "    plot_training_history(train_losses, val_losses, final_test_loss, num_epochs, config)\n",
    "\n",
    "    return model  # Return the trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24e6a7be-1326-4893-9b36-35b652376348",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNetRegression(\n",
      "  (conv_layers): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(2, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): ReLU()\n",
      "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): ReLU()\n",
      "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (skip_ops): ModuleDict(\n",
      "    (0->2): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    )\n",
      "    (2->4): Identity()\n",
      "  )\n",
      "  (fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=32768, out_features=64, bias=True)\n",
      "    (1): Linear(in_features=64, out_features=64, bias=True)\n",
      "  )\n",
      "  (fc_out): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Set device to GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize model and move to device (GPU/CPU)\n",
    "model = ResNetRegression(config=config, num_outputs=len(to_predict)).to(device)\n",
    "\n",
    "# Initialize optimizer based on config\n",
    "optimizer_name = config[\"training\"][\"optimizer\"]\n",
    "optimizer = getattr(optim, optimizer_name)(\n",
    "    model.parameters(), lr=config[\"training\"][\"learning_rate\"]\n",
    ")\n",
    "\n",
    "# Get number of epochs from config\n",
    "num_epochs = config[\"training\"][\"num_epochs\"]\n",
    "\n",
    "# Initialize loss function\n",
    "criterion = CustomRMSELoss()\n",
    "\n",
    "# Print model architecture\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b32b72-6fb1-482d-98e8-67fb38ca377b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training ResNet model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 Training: 100%|██████████| 3/3 [00:00<00:00,  3.72it/s, loss=5.3410]\n",
      "Epoch 1/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 22.45it/s, loss=5.0590]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50 Summary:\n",
      "Training Loss: 4.9889\n",
      "Validation Loss: 5.0590\n",
      "Average Loss: 5.0239\n",
      "\n",
      "Saved new best model with validation loss: 5.0590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 Training: 100%|██████████| 3/3 [00:00<00:00,  9.31it/s, loss=4.8048]\n",
      "Epoch 2/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 21.88it/s, loss=4.6022]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/50 Summary:\n",
      "Training Loss: 4.8414\n",
      "Validation Loss: 4.6022\n",
      "Average Loss: 4.7218\n",
      "\n",
      "Saved new best model with validation loss: 4.6022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 Training: 100%|██████████| 3/3 [00:00<00:00,  9.70it/s, loss=4.4451]\n",
      "Epoch 3/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 26.89it/s, loss=3.5099]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/50 Summary:\n",
      "Training Loss: 4.4416\n",
      "Validation Loss: 3.5099\n",
      "Average Loss: 3.9758\n",
      "\n",
      "Saved new best model with validation loss: 3.5099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 Training: 100%|██████████| 3/3 [00:00<00:00,  8.18it/s, loss=2.8903]\n",
      "Epoch 4/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 24.50it/s, loss=2.4736]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/50 Summary:\n",
      "Training Loss: 3.2146\n",
      "Validation Loss: 2.4736\n",
      "Average Loss: 2.8441\n",
      "\n",
      "Saved new best model with validation loss: 2.4736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 Training: 100%|██████████| 3/3 [00:00<00:00,  8.83it/s, loss=2.2692]\n",
      "Epoch 5/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 25.73it/s, loss=1.1863]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/50 Summary:\n",
      "Training Loss: 2.5272\n",
      "Validation Loss: 1.1863\n",
      "Average Loss: 1.8568\n",
      "\n",
      "Saved new best model with validation loss: 1.1863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 Training: 100%|██████████| 3/3 [00:00<00:00, 10.29it/s, loss=0.9144]\n",
      "Epoch 6/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 25.27it/s, loss=5.0718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/50 Summary:\n",
      "Training Loss: 1.6120\n",
      "Validation Loss: 5.0718\n",
      "Average Loss: 3.3419\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 Training: 100%|██████████| 3/3 [00:00<00:00,  8.93it/s, loss=1.7977]\n",
      "Epoch 7/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 25.80it/s, loss=3.2095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/50 Summary:\n",
      "Training Loss: 3.1355\n",
      "Validation Loss: 3.2095\n",
      "Average Loss: 3.1725\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 Training: 100%|██████████| 3/3 [00:00<00:00,  9.93it/s, loss=1.9148]\n",
      "Epoch 8/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 28.63it/s, loss=0.9934]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/50 Summary:\n",
      "Training Loss: 2.6046\n",
      "Validation Loss: 0.9934\n",
      "Average Loss: 1.7990\n",
      "\n",
      "Saved new best model with validation loss: 0.9934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 Training: 100%|██████████| 3/3 [00:00<00:00,  9.71it/s, loss=2.0761]\n",
      "Epoch 9/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 26.07it/s, loss=2.2874]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/50 Summary:\n",
      "Training Loss: 1.4656\n",
      "Validation Loss: 2.2874\n",
      "Average Loss: 1.8765\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 Training: 100%|██████████| 3/3 [00:00<00:00,  9.84it/s, loss=2.1567]\n",
      "Epoch 10/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 21.67it/s, loss=1.5039]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/50 Summary:\n",
      "Training Loss: 2.1069\n",
      "Validation Loss: 1.5039\n",
      "Average Loss: 1.8054\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 Training: 100%|██████████| 3/3 [00:00<00:00, 10.11it/s, loss=0.8998]\n",
      "Epoch 11/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 27.04it/s, loss=1.0730]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/50 Summary:\n",
      "Training Loss: 1.4714\n",
      "Validation Loss: 1.0730\n",
      "Average Loss: 1.2722\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 Training: 100%|██████████| 3/3 [00:00<00:00,  8.92it/s, loss=1.4773]\n",
      "Epoch 12/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 23.70it/s, loss=1.3313]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/50 Summary:\n",
      "Training Loss: 1.3230\n",
      "Validation Loss: 1.3313\n",
      "Average Loss: 1.3272\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50 Training: 100%|██████████| 3/3 [00:00<00:00,  9.54it/s, loss=1.0964]\n",
      "Epoch 13/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 22.12it/s, loss=1.2430]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/50 Summary:\n",
      "Training Loss: 1.2925\n",
      "Validation Loss: 1.2430\n",
      "Average Loss: 1.2678\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50 Training: 100%|██████████| 3/3 [00:00<00:00,  9.26it/s, loss=1.6602]\n",
      "Epoch 14/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 25.35it/s, loss=1.2517]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/50 Summary:\n",
      "Training Loss: 1.2278\n",
      "Validation Loss: 1.2517\n",
      "Average Loss: 1.2397\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50 Training: 100%|██████████| 3/3 [00:00<00:00,  8.68it/s, loss=1.1451]\n",
      "Epoch 15/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 21.52it/s, loss=0.8671]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/50 Summary:\n",
      "Training Loss: 1.0749\n",
      "Validation Loss: 0.8671\n",
      "Average Loss: 0.9710\n",
      "\n",
      "Saved new best model with validation loss: 0.8671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50 Training: 100%|██████████| 3/3 [00:00<00:00,  9.78it/s, loss=0.9053]\n",
      "Epoch 16/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 22.50it/s, loss=0.6669]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16/50 Summary:\n",
      "Training Loss: 0.9541\n",
      "Validation Loss: 0.6669\n",
      "Average Loss: 0.8105\n",
      "\n",
      "Saved new best model with validation loss: 0.6669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50 Training: 100%|██████████| 3/3 [00:00<00:00,  9.38it/s, loss=0.6436]\n",
      "Epoch 17/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 25.02it/s, loss=0.7382]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/50 Summary:\n",
      "Training Loss: 0.8104\n",
      "Validation Loss: 0.7382\n",
      "Average Loss: 0.7743\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50 Training: 100%|██████████| 3/3 [00:00<00:00,  9.41it/s, loss=0.7400]\n",
      "Epoch 18/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 25.60it/s, loss=0.6543]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18/50 Summary:\n",
      "Training Loss: 0.7124\n",
      "Validation Loss: 0.6543\n",
      "Average Loss: 0.6834\n",
      "\n",
      "Saved new best model with validation loss: 0.6543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50 Training: 100%|██████████| 3/3 [00:00<00:00,  9.55it/s, loss=0.5527]\n",
      "Epoch 19/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 25.81it/s, loss=0.7134]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19/50 Summary:\n",
      "Training Loss: 0.7495\n",
      "Validation Loss: 0.7134\n",
      "Average Loss: 0.7315\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50 Training: 100%|██████████| 3/3 [00:00<00:00,  9.35it/s, loss=0.4374]\n",
      "Epoch 20/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 25.40it/s, loss=0.7914]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20/50 Summary:\n",
      "Training Loss: 0.6668\n",
      "Validation Loss: 0.7914\n",
      "Average Loss: 0.7291\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50 Training: 100%|██████████| 3/3 [00:00<00:00,  9.57it/s, loss=0.5109]\n",
      "Epoch 21/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 25.47it/s, loss=0.8351]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21/50 Summary:\n",
      "Training Loss: 0.6491\n",
      "Validation Loss: 0.8351\n",
      "Average Loss: 0.7421\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50 Training: 100%|██████████| 3/3 [00:00<00:00,  9.56it/s, loss=0.8655]\n",
      "Epoch 22/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 28.05it/s, loss=0.3309]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22/50 Summary:\n",
      "Training Loss: 0.8292\n",
      "Validation Loss: 0.3309\n",
      "Average Loss: 0.5801\n",
      "\n",
      "Saved new best model with validation loss: 0.3309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50 Training: 100%|██████████| 3/3 [00:00<00:00,  9.20it/s, loss=0.8509]\n",
      "Epoch 23/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 25.10it/s, loss=0.3911]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23/50 Summary:\n",
      "Training Loss: 0.6235\n",
      "Validation Loss: 0.3911\n",
      "Average Loss: 0.5073\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50 Training: 100%|██████████| 3/3 [00:00<00:00,  9.45it/s, loss=0.6079]\n",
      "Epoch 24/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 25.24it/s, loss=0.4997]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24/50 Summary:\n",
      "Training Loss: 0.5342\n",
      "Validation Loss: 0.4997\n",
      "Average Loss: 0.5170\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50 Training: 100%|██████████| 3/3 [00:00<00:00,  9.21it/s, loss=0.7167]\n",
      "Epoch 25/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 25.14it/s, loss=0.5975]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25/50 Summary:\n",
      "Training Loss: 0.5545\n",
      "Validation Loss: 0.5975\n",
      "Average Loss: 0.5760\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50 Training: 100%|██████████| 3/3 [00:00<00:00,  9.12it/s, loss=0.4602]\n",
      "Epoch 26/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 24.06it/s, loss=0.3440]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26/50 Summary:\n",
      "Training Loss: 0.4191\n",
      "Validation Loss: 0.3440\n",
      "Average Loss: 0.3815\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50 Training: 100%|██████████| 3/3 [00:00<00:00,  9.33it/s, loss=0.5845]\n",
      "Epoch 27/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 24.95it/s, loss=0.3208]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27/50 Summary:\n",
      "Training Loss: 0.4087\n",
      "Validation Loss: 0.3208\n",
      "Average Loss: 0.3647\n",
      "\n",
      "Saved new best model with validation loss: 0.3208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50 Training: 100%|██████████| 3/3 [00:00<00:00,  9.12it/s, loss=0.4121]\n",
      "Epoch 28/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 25.52it/s, loss=0.3855]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28/50 Summary:\n",
      "Training Loss: 0.3729\n",
      "Validation Loss: 0.3855\n",
      "Average Loss: 0.3792\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50 Training: 100%|██████████| 3/3 [00:00<00:00,  8.33it/s, loss=0.2650]\n",
      "Epoch 29/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 23.06it/s, loss=0.4576]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29/50 Summary:\n",
      "Training Loss: 0.2870\n",
      "Validation Loss: 0.4576\n",
      "Average Loss: 0.3723\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50 Training: 100%|██████████| 3/3 [00:00<00:00,  8.65it/s, loss=0.2856]\n",
      "Epoch 30/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 22.39it/s, loss=0.3331]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30/50 Summary:\n",
      "Training Loss: 0.3859\n",
      "Validation Loss: 0.3331\n",
      "Average Loss: 0.3595\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50 Training: 100%|██████████| 3/3 [00:00<00:00,  8.92it/s, loss=0.3149]\n",
      "Epoch 31/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 20.56it/s, loss=0.2767]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31/50 Summary:\n",
      "Training Loss: 0.2899\n",
      "Validation Loss: 0.2767\n",
      "Average Loss: 0.2833\n",
      "\n",
      "Saved new best model with validation loss: 0.2767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50 Training: 100%|██████████| 3/3 [00:00<00:00,  7.80it/s, loss=0.4214]\n",
      "Epoch 32/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 24.32it/s, loss=0.2596]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32/50 Summary:\n",
      "Training Loss: 0.3855\n",
      "Validation Loss: 0.2596\n",
      "Average Loss: 0.3225\n",
      "\n",
      "Saved new best model with validation loss: 0.2596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50 Training: 100%|██████████| 3/3 [00:00<00:00,  8.16it/s, loss=0.1296]\n",
      "Epoch 33/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 24.17it/s, loss=0.5264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33/50 Summary:\n",
      "Training Loss: 0.2348\n",
      "Validation Loss: 0.5264\n",
      "Average Loss: 0.3806\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50 Training: 100%|██████████| 3/3 [00:00<00:00,  9.63it/s, loss=0.1755]\n",
      "Epoch 34/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 24.50it/s, loss=0.4537]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34/50 Summary:\n",
      "Training Loss: 0.3366\n",
      "Validation Loss: 0.4537\n",
      "Average Loss: 0.3951\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50 Training: 100%|██████████| 3/3 [00:00<00:00,  9.62it/s, loss=0.3182]\n",
      "Epoch 35/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 24.23it/s, loss=0.2451]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35/50 Summary:\n",
      "Training Loss: 0.3520\n",
      "Validation Loss: 0.2451\n",
      "Average Loss: 0.2986\n",
      "\n",
      "Saved new best model with validation loss: 0.2451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50 Training: 100%|██████████| 3/3 [00:00<00:00,  9.54it/s, loss=0.3515]\n",
      "Epoch 36/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 23.78it/s, loss=0.3206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36/50 Summary:\n",
      "Training Loss: 0.2612\n",
      "Validation Loss: 0.3206\n",
      "Average Loss: 0.2909\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50 Training: 100%|██████████| 3/3 [00:00<00:00,  9.75it/s, loss=0.2415]\n",
      "Epoch 37/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 25.47it/s, loss=0.2552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37/50 Summary:\n",
      "Training Loss: 0.2506\n",
      "Validation Loss: 0.2552\n",
      "Average Loss: 0.2529\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50 Training: 100%|██████████| 3/3 [00:00<00:00,  9.22it/s, loss=0.0951]\n",
      "Epoch 38/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 22.65it/s, loss=0.1551]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38/50 Summary:\n",
      "Training Loss: 0.1972\n",
      "Validation Loss: 0.1551\n",
      "Average Loss: 0.1762\n",
      "\n",
      "Saved new best model with validation loss: 0.1551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50 Training: 100%|██████████| 3/3 [00:00<00:00, 10.22it/s, loss=0.2368]\n",
      "Epoch 39/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 26.26it/s, loss=0.1724]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39/50 Summary:\n",
      "Training Loss: 0.1774\n",
      "Validation Loss: 0.1724\n",
      "Average Loss: 0.1749\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50 Training: 100%|██████████| 3/3 [00:00<00:00,  9.78it/s, loss=0.1834]\n",
      "Epoch 40/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 20.83it/s, loss=0.2857]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40/50 Summary:\n",
      "Training Loss: 0.1801\n",
      "Validation Loss: 0.2857\n",
      "Average Loss: 0.2329\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50 Training: 100%|██████████| 3/3 [00:00<00:00,  7.92it/s, loss=0.1365]\n",
      "Epoch 41/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 22.82it/s, loss=0.5952]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 41/50 Summary:\n",
      "Training Loss: 0.1759\n",
      "Validation Loss: 0.5952\n",
      "Average Loss: 0.3855\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50 Training: 100%|██████████| 3/3 [00:00<00:00,  8.34it/s, loss=0.7296]\n",
      "Epoch 42/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 22.54it/s, loss=0.5034]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42/50 Summary:\n",
      "Training Loss: 0.5910\n",
      "Validation Loss: 0.5034\n",
      "Average Loss: 0.5472\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50 Training: 100%|██████████| 3/3 [00:00<00:00,  9.49it/s, loss=0.4097]\n",
      "Epoch 43/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 24.60it/s, loss=0.5011]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 43/50 Summary:\n",
      "Training Loss: 0.3321\n",
      "Validation Loss: 0.5011\n",
      "Average Loss: 0.4166\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50 Training: 100%|██████████| 3/3 [00:00<00:00,  8.46it/s, loss=0.2257]\n",
      "Epoch 44/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 22.83it/s, loss=0.4101]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44/50 Summary:\n",
      "Training Loss: 0.2531\n",
      "Validation Loss: 0.4101\n",
      "Average Loss: 0.3316\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50 Training: 100%|██████████| 3/3 [00:00<00:00,  9.24it/s, loss=0.1941]\n",
      "Epoch 45/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 23.22it/s, loss=0.4263]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 45/50 Summary:\n",
      "Training Loss: 0.2951\n",
      "Validation Loss: 0.4263\n",
      "Average Loss: 0.3607\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50 Training: 100%|██████████| 3/3 [00:00<00:00,  9.39it/s, loss=0.2130]\n",
      "Epoch 46/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 24.46it/s, loss=0.2554]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46/50 Summary:\n",
      "Training Loss: 0.2948\n",
      "Validation Loss: 0.2554\n",
      "Average Loss: 0.2751\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50 Training: 100%|██████████| 3/3 [00:00<00:00,  9.09it/s, loss=0.2795]\n",
      "Epoch 47/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 20.84it/s, loss=0.1155]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 47/50 Summary:\n",
      "Training Loss: 0.3399\n",
      "Validation Loss: 0.1155\n",
      "Average Loss: 0.2277\n",
      "\n",
      "Saved new best model with validation loss: 0.1155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50 Training: 100%|██████████| 3/3 [00:00<00:00,  8.80it/s, loss=0.4109]\n",
      "Epoch 48/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 19.85it/s, loss=0.3124]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48/50 Summary:\n",
      "Training Loss: 0.2489\n",
      "Validation Loss: 0.3124\n",
      "Average Loss: 0.2807\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50 Training: 100%|██████████| 3/3 [00:00<00:00,  8.25it/s, loss=0.2806]\n",
      "Epoch 49/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 24.47it/s, loss=0.2389]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 49/50 Summary:\n",
      "Training Loss: 0.2102\n",
      "Validation Loss: 0.2389\n",
      "Average Loss: 0.2246\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 Training: 100%|██████████| 3/3 [00:00<00:00,  8.55it/s, loss=0.1329]\n",
      "Epoch 50/50 Validation: 100%|██████████| 1/1 [00:00<00:00, 24.68it/s, loss=0.3940]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 50/50 Summary:\n",
      "Training Loss: 0.2907\n",
      "Validation Loss: 0.3940\n",
      "Average Loss: 0.3423\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Best Model: 100%|██████████| 1/1 [00:00<00:00, 24.79it/s, loss=0.1545]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Loss: 0.1545\n"
     ]
    }
   ],
   "source": [
    "# Get number of epochs from config\n",
    "num_epochs = config[\"training\"][\"num_epochs\"]\n",
    "\n",
    "# Print message indicating training start\n",
    "print(\"\\nTraining ResNet model...\")\n",
    "\n",
    "# Train and evaluate the model\n",
    "# Calls the train_and_evaluate function with model, data loaders, optimizer, and loss function\n",
    "model = train_and_evaluate(model, train_loader, val_loader, test_loader, num_epochs, optimizer, criterion, device, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93c3af2-9e4c-4935-ace5-cad4acaa00ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fa0961-6d40-4faf-9f4c-b7a048d2af73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22adb8a7-d1ed-4357-91ae-2219fab1cf6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9d895f-f75d-4362-9c27-70b26eb3b223",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
