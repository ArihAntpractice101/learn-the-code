{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e169314-de31-4766-ad3f-203326067c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # Main PyTorch package for tensor operations and deep learning functionalities\n",
    "import torch.nn as nn  # Submodule for defining neural network layers like nn.Linear, nn.Conv2d, etc.\n",
    "import torch.optim as optim  # Optimizers such as Adam, SGD for model training\n",
    "from torch.utils.data import Dataset, DataLoader, random_split  # Dataset: abstract class for data, DataLoader: batching, random_split: dataset partitioning\n",
    "import numpy as np  # Numerical operations and array manipulation, often used for preprocessing and conversion\n",
    "from pymongo import MongoClient  # Interface for connecting to MongoDB — used to retrieve or store PSF data and labels\n",
    "from tqdm import tqdm  # Displays smart progress bars for training/validation loops and batch iterations\n",
    "import copy  # Enables deep copying of model weights (useful for checkpointing the best model)\n",
    "import matplotlib.pyplot as plt  # Plotting utility for visualizations like training/validation loss curves and PSF comparisons\n",
    "import warnings  # Python’s standard warning control module — used here to suppress non-critical warnings\n",
    "import torch.nn.functional as F  # Provides stateless versions of activation functions and loss functions (e.g., F.relu, F.mse_loss)\n",
    "import os  # OS interface to create directories, manage file paths, and access environment-specific variables\n",
    "import yaml  # Parses YAML configuration files into Python dictionaries (used if OmegaConf isn’t applied)\n",
    "from omegaconf import OmegaConf  # More powerful configuration handler than `yaml` — supports dot-access and structured configs\n",
    "from omegaconf import DictConfig, ListConfig  # Typed configuration containers supporting dot access and validation\n",
    "import random  # Python’s built-in random module — used for global seeding and randomness control in splits or shuffling\n",
    "\n",
    "warnings.filterwarnings('ignore')  # Silences all warning messages to keep console/log outputs clean during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38f99c89-dabb-4b23-95df-1ddf9c9d49ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")  # non-GUI backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bd05104-2db2-4b6f-8e13-0af19e4ff340",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22258721-e43c-489a-8e94-dfc11d32309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")   # no GUI, no crashes\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "993551eb-b1d1-410c-8afc-ae5b9da67762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the dataset configuration file exists\n",
    "if not os.path.exists('./dataset_config.yaml'):\n",
    "    raise FileNotFoundError(\"Dataset config file not found.\")\n",
    "\n",
    "# Check if the model configuration file exists\n",
    "if not os.path.exists('./unet.yaml'):\n",
    "    raise FileNotFoundError(\"Model config file not found.\")\n",
    "\n",
    "# Load dataset configuration from a YAML file into an OmegaConf object\n",
    "dataset_config = OmegaConf.load('./dataset_config.yaml')  # Contains settings like dataset paths, preprocessing, seed, etc.\n",
    "\n",
    "# Load model architecture and training configuration from another YAML file\n",
    "model_config = OmegaConf.load('./unet.yaml')  # Contains model details like layers, optimizer settings, and training hyperparameters\n",
    "\n",
    "# Merge the dataset and model configs into a single config dictionary-like object\n",
    "config = OmegaConf.merge(dataset_config, model_config)  # Allows unified access to all config parameters using dot notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7da3fe63-6a8c-4530-aeed-7ee8c22d4fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate MongoDB configuration\n",
    "if 'mongodb' not in config:\n",
    "    raise KeyError(\"MongoDB configuration section is missing in the configuration file.\")\n",
    "\n",
    "if 'uri' not in config['mongodb']:\n",
    "    raise KeyError(\"MongoDB URI is missing in the configuration file.\")\n",
    "if 'database' not in config['mongodb']:\n",
    "    raise KeyError(\"MongoDB database name is missing in the configuration file.\")\n",
    "if 'collection' not in config['mongodb']:\n",
    "    raise KeyError(\"MongoDB collection name is missing in the configuration file.\")\n",
    "\n",
    "if not isinstance(config['mongodb']['uri'], str):\n",
    "    raise TypeError(\"MongoDB URI must be a string.\")\n",
    "if not isinstance(config['mongodb']['database'], str):\n",
    "    raise TypeError(\"MongoDB database name must be a string.\")\n",
    "if not isinstance(config['mongodb']['collection'], str):\n",
    "    raise TypeError(\"MongoDB collection name must be a string.\")\n",
    "\n",
    "# Attempt to connect to MongoDB and create index\n",
    "try:\n",
    "    client = MongoClient(config['mongodb']['uri'])  # Connect to MongoDB using the URI\n",
    "    db = client[config['mongodb']['database']]  # Access the database\n",
    "    full_collection = db[config['mongodb']['collection']]  # Access the collection\n",
    "    full_collection.create_index(\"index\", name=\"idx\", unique=True)  # Ensure unique index field\n",
    "except Exception as e:\n",
    "    raise ConnectionError(f\"Failed to connect to MongoDB or create index: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "750fd199-eb57-46f1-9746-0b2bfccec3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate and set seeds for reproducibility\n",
    "if \"dataset\" not in config:\n",
    "    raise KeyError(\"Missing 'dataset' section in the configuration.\")\n",
    "if 'seed' not in config['dataset']:\n",
    "    raise KeyError(\"Seed value is missing from the configuration under 'dataset' section.\")\n",
    "if not isinstance(config['dataset']['seed'], int):\n",
    "    raise TypeError(\"`seed` value must be an integer.\")\n",
    "\n",
    "seed = config[\"dataset\"][\"seed\"]\n",
    "\n",
    "if not isinstance(seed, int):\n",
    "    raise TypeError(f\"Seed must be an integer, got {type(seed)} instead.\")\n",
    "\n",
    "# Set seeds across all relevant libraries\n",
    "random.seed(seed)  # Python's built-in random\n",
    "np.random.seed(seed)  # NumPy random\n",
    "torch.manual_seed(seed)  # PyTorch CPU RNG\n",
    "torch.cuda.manual_seed_all(seed)  # PyTorch CUDA RNGs (all GPUs)\n",
    "torch.backends.cudnn.deterministic = True  # Force deterministic behavior in cuDNN\n",
    "torch.backends.cudnn.benchmark = False  # Disable auto-tuning for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0093fd44-f09b-4a1d-b5b6-3511dde371b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_mirrors(config):\n",
    "    \"\"\"\n",
    "    Validate the mirror configurations in the config file.\n",
    "\n",
    "    This function checks that all required mirror segments and their parameters are properly defined.\n",
    "    It verifies that each segment has the necessary piston, tip, and tilt parameters and that \n",
    "    they are correctly formatted.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    config : dict\n",
    "        Configuration dictionary loaded from the YAML file\n",
    "        \n",
    "    Raises:\n",
    "    -------\n",
    "    ValueError\n",
    "        If any mirror configuration is invalid or missing\n",
    "    \"\"\"\n",
    "    # Validate primary mirror configuration\n",
    "    if 'primary_mirror' not in config:\n",
    "        raise ValueError(\"Missing 'primary_mirror' section in config.\")\n",
    "\n",
    "    # Check if 'segments' exists in the primary_mirror section\n",
    "    if 'segments' not in config['primary_mirror']:\n",
    "        raise ValueError(\"Missing 'segments' key in 'primary_mirror' section.\")\n",
    "\n",
    "    segments = config['primary_mirror']['segments']\n",
    "    # Use isinstance with (list, ListConfig) to handle both Python lists and OmegaConf ListConfig objects\n",
    "    if not isinstance(segments, (list, ListConfig)):\n",
    "        raise ValueError(\"'segments' under 'primary_mirror' must be a list.\")\n",
    "\n",
    "    found_ids = set()\n",
    "    missing_params_by_id = {}\n",
    "\n",
    "    # Validate each segment in the primary mirror\n",
    "    for i, segment in enumerate(segments):\n",
    "        # Validate segment has an ID\n",
    "        if 'id' not in segment:\n",
    "            raise ValueError(f\"Segment at index {i} is missing 'id' key.\")\n",
    "        \n",
    "        seg_id = segment['id']\n",
    "        if not isinstance(seg_id, int):\n",
    "            raise ValueError(f\"Segment at index {i} has a non-integer 'id': {seg_id}\")\n",
    "        \n",
    "        # Check for duplicate IDs\n",
    "        if seg_id in found_ids:\n",
    "            raise ValueError(f\"Duplicate segment ID found: {seg_id}\")\n",
    "        \n",
    "        found_ids.add(seg_id)\n",
    "\n",
    "        # Check for required parameters (piston, tip, tilt)\n",
    "        missing_keys = []\n",
    "        for key in ['piston', 'tip', 'tilt']:\n",
    "            if key not in segment:\n",
    "                missing_keys.append(key)\n",
    "                continue\n",
    "\n",
    "            # Validate parameter format (must be a dict with either 'static', 'range', or 'untrained_range')\n",
    "            param_dict = segment[key]\n",
    "            if not isinstance(param_dict, (dict, DictConfig)):\n",
    "                raise ValueError(f\"{key} for segment {seg_id} must be a dictionary.\")\n",
    "            \n",
    "            # Check that exactly one of the allowed keys is present\n",
    "            allowed_keys = {'static', 'range', 'untrained_range'}\n",
    "            param_keys = set(param_dict.keys())\n",
    "            if not param_keys.issubset(allowed_keys):\n",
    "                invalid_keys = param_keys - allowed_keys\n",
    "                raise ValueError(f\"{key} for segment {seg_id} contains invalid keys: {invalid_keys}. \"\n",
    "                               f\"Must use only: {allowed_keys}.\")\n",
    "            \n",
    "            if len(param_dict) != 1:\n",
    "                raise ValueError(f\"{key} for segment {seg_id} must contain exactly one of {allowed_keys}.\")\n",
    "            \n",
    "            # Get the parameter type (static or range)\n",
    "            inner_key = next(iter(param_dict))\n",
    "            \n",
    "            # Validate the value based on parameter type\n",
    "            value = param_dict[inner_key]\n",
    "            if inner_key == 'static':\n",
    "                # Static values must be floats\n",
    "                if not isinstance(value, (int, float)):\n",
    "                    raise ValueError(f\"{key} for segment {seg_id} must be a number under 'static', but got {type(value).__name__}.\")\n",
    "            else:\n",
    "                # Range values must be a list of exactly 2 floats\n",
    "                if not isinstance(value, (list, ListConfig)):\n",
    "                    raise ValueError(f\"{key} for segment {seg_id} under '{inner_key}' must be a list, but got {type(value).__name__}.\")\n",
    "                \n",
    "                if len(value) != 2:\n",
    "                    raise ValueError(f\"{key} for segment {seg_id} under '{inner_key}' must be a list of exactly 2 values.\")\n",
    "                \n",
    "                if not all(isinstance(v, (int, float)) for v in value):\n",
    "                    non_numeric = [i for i, v in enumerate(value) if not isinstance(v, (int, float))]\n",
    "                    raise ValueError(f\"{key} for segment {seg_id} under '{inner_key}' contains non-numeric values at positions {non_numeric}.\")\n",
    "\n",
    "        # Track any missing parameters for this segment\n",
    "        if missing_keys:\n",
    "            missing_params_by_id[seg_id] = missing_keys\n",
    "\n",
    "    # Ensure all required segments (1-6) are present\n",
    "    required_ids = set(range(1, 7))\n",
    "    missing_ids = sorted(list(required_ids - found_ids))\n",
    "    if missing_ids:\n",
    "        raise ValueError(f\"Missing segments with IDs: {missing_ids}\")\n",
    "\n",
    "    # Report any segments with missing parameters\n",
    "    if missing_params_by_id:\n",
    "        msg = \"Some segments are missing required parameters:\\n\"\n",
    "        for sid, keys in missing_params_by_id.items():\n",
    "            msg += f\"  Segment ID {sid}: missing {', '.join(keys)}\\n\"\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    # Validate secondary mirror configuration\n",
    "    if 'secondary_mirror' not in config:\n",
    "        raise ValueError(\"Missing 'secondary_mirror' section in config.\")\n",
    "\n",
    "    secondary_params = config['secondary_mirror']\n",
    "\n",
    "    # Check that secondary mirror has all required parameters\n",
    "    for param in ['piston', 'tip', 'tilt']:\n",
    "        if param not in secondary_params:\n",
    "            raise ValueError(f\"Missing '{param}' parameter in 'secondary_mirror'.\")\n",
    "\n",
    "        # Validate parameter format\n",
    "        param_dict = secondary_params[param]\n",
    "        if not isinstance(param_dict, (dict, DictConfig)):\n",
    "            raise ValueError(f\"{param} for secondary mirror must be a dictionary, but got {type(param_dict).__name__}.\")\n",
    "        \n",
    "        # Check that exactly one of the allowed keys is present\n",
    "        allowed_keys = {'static', 'range', 'untrained_range'}\n",
    "        param_keys = set(param_dict.keys())\n",
    "        if not param_keys.issubset(allowed_keys):\n",
    "            invalid_keys = param_keys - allowed_keys\n",
    "            raise ValueError(f\"{param} for secondary mirror contains invalid keys: {invalid_keys}. \"\n",
    "                           f\"Must use only: {allowed_keys}.\")\n",
    "        \n",
    "        if len(param_dict) != 1:\n",
    "            raise ValueError(f\"{param} for secondary mirror must contain exactly one of {allowed_keys}.\")\n",
    "        \n",
    "        # Get the parameter type\n",
    "        inner_key = next(iter(param_dict))\n",
    "        \n",
    "        # Validate the value based on parameter type\n",
    "        value = param_dict[inner_key]\n",
    "        if inner_key == 'static':\n",
    "            # Static values must be numbers (int or float)\n",
    "            if not isinstance(value, (int, float)):\n",
    "                raise ValueError(f\"{param} for secondary mirror must be a number under 'static', but got {type(value).__name__}.\")\n",
    "        else:\n",
    "            # Range values must be a list of two numbers\n",
    "            if not isinstance(value, (list, ListConfig)):\n",
    "                raise ValueError(f\"{param} for secondary mirror under '{inner_key}' must be a list, but got {type(value).__name__}.\")\n",
    "            \n",
    "            if len(value) != 2:\n",
    "                raise ValueError(f\"{param} for secondary mirror under '{inner_key}' must be a list of exactly 2 values.\")\n",
    "            \n",
    "            if not all(isinstance(v, (int, float)) for v in value):\n",
    "                non_numeric = [i for i, v in enumerate(value) if not isinstance(v, (int, float))]\n",
    "                raise ValueError(f\"{param} for secondary mirror under '{inner_key}' contains non-numeric values at positions {non_numeric}.\")\n",
    "\n",
    "    return True  # Return True if validation passes\n",
    "\n",
    "validate_mirrors(config);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4ffa538-ef96-4973-a0fd-e3beb955185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_parameters(config):\n",
    "    # Lists to categorize parameters based on training roles\n",
    "    to_predict = []           # Parameters with defined \"range\" — included in training\n",
    "    untrained_predict = []    # Parameters with \"untrained_range\" — excluded from training, used for testing/generalization\n",
    "    not_to_predict = []       # Parameters marked \"static\" — constant values, not predicted\n",
    "\n",
    "    # Iterate through each segment in the primary mirror\n",
    "    for segment in config[\"primary_mirror\"][\"segments\"]:\n",
    "        seg_id = segment[\"id\"]  # Segment identifier (e.g., 1, 2, 3...)\n",
    "\n",
    "        for param in [\"piston\", \"tip\", \"tilt\"]:  # Loop through each degree of freedom (DoF)\n",
    "            param_config = segment[param]  # Access configuration for this DoF\n",
    "            param_name = f\"primary_{seg_id}_{param}\"  # Create parameter name, e.g., \"primary_1_piston\"\n",
    "\n",
    "            # Classify the parameter into the appropriate category\n",
    "            if \"range\" in param_config:\n",
    "                to_predict.append(param_name)  # Included in model training\n",
    "            elif \"untrained_range\" in param_config:\n",
    "                untrained_predict.append(param_name)  # Held out for generalization testing\n",
    "            elif \"static\" in param_config:\n",
    "                not_to_predict.append(param_name)  # Fixed parameter, excluded from prediction\n",
    "\n",
    "    # Process secondary mirror parameters in the same way\n",
    "    for param in [\"piston\", \"tip\", \"tilt\"]:\n",
    "        param_config = config[\"secondary_mirror\"][param]  # Access config for each DoF\n",
    "        param_name = f\"secondary_{param}\"  # e.g., \"secondary_tip\"\n",
    "\n",
    "        # Classify the secondary mirror parameter\n",
    "        if \"range\" in param_config:\n",
    "            to_predict.append(param_name)\n",
    "        elif \"untrained_range\" in param_config:\n",
    "            untrained_predict.append(param_name)\n",
    "        elif \"static\" in param_config:\n",
    "            not_to_predict.append(param_name)\n",
    "\n",
    "    # Return all categorized parameter names\n",
    "    return to_predict, untrained_predict, not_to_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b950eb7d-ba80-4e7f-a962-9a5a95994930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['secondary_piston']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_predict, untrained_predict, not_to_predict = extract_parameters(config)\n",
    "to_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ccf40fc-fb19-47b7-8622-71c546215f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to flatten a nested dictionary into a flat dictionary\n",
    "def flatten_dict(d, parent_key='', sep='_'):\n",
    "    items = []  # Initialize an empty list to store the flattened key-value pairs\n",
    "    \n",
    "    for k, v in d.items():  # Iterate through each key-value pair in the dictionary\n",
    "        # If a parent_key exists, join it with the current key using the separator\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k  # Construct the flattened key\n",
    "        \n",
    "        if isinstance(v, dict):  # Check if the value is a dictionary\n",
    "            # Recursively flatten the nested dictionary and extend the result to the items list\n",
    "            items.extend(flatten_dict(v, new_key, sep=sep).items())  # Flatten the nested dict and add to the list\n",
    "        else:\n",
    "            # If the value is not a dictionary, add the key-value pair directly to the list\n",
    "            items.append((new_key, v))  # Add the key-value pair to the flattened list\n",
    "    \n",
    "    return dict(items)  # Return the flattened dictionary as a dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff4209ac-5354-4490-aaff-276785d4d6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBImageDataset(Dataset):\n",
    "    def __init__(self, config, shuffle=True):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with merged config and shuffle flag.\n",
    "\n",
    "        :param config: Merged configuration object\n",
    "        :param shuffle: Flag to control shuffling of dataset\n",
    "        \"\"\"\n",
    "        self.config = config  # Store the dataset configuration\n",
    "        self.mongo_collection = full_collection  # MongoDB collection for fetching documents (must be pre-initialized)\n",
    "        self.target_keys = to_predict  # Target parameters that the model should predict\n",
    "        self.shuffle = shuffle  # Flag for whether to shuffle the dataset on initialization\n",
    "\n",
    "        # --- Defocus Validation and Handling ---\n",
    "        raw_defocus = self.config[\"dataset\"][\"use_defocus\"]  # Raw value from config (can be int or float)\n",
    "\n",
    "        # Ensure that 'use_defocus' is a numeric type (int or float)\n",
    "        if not isinstance(raw_defocus, (int, float)):\n",
    "            raise TypeError(f\"'use_defocus' must be an int or float, got {type(raw_defocus)}.\")\n",
    "        \n",
    "        # Convert to boolean: 0 means no defocus used, non-zero means defocus applied\n",
    "        self.use_defocused = bool(raw_defocus)\n",
    "\n",
    "        # --- Transform Validation ---\n",
    "        self.transform = self.config[\"dataset\"][\"transform\"]  # Type of transform to apply on PSFs\n",
    "        allowed_transforms = {\"linear\", \"sqrt\", \"log\"}  # Only these three are allowed\n",
    "        \n",
    "        # Validate that transform type is one of the supported options\n",
    "        if self.transform not in allowed_transforms:\n",
    "            raise ValueError(f\"Unsupported transform '{self.transform}'. Must be one of {allowed_transforms}.\")\n",
    "\n",
    "        # --- Load Dataset Indexes ---\n",
    "        # Fetch all document indexes from the MongoDB collection (used to retrieve samples efficiently)\n",
    "        self.doc_indexes = list(\n",
    "            self.mongo_collection.find({}, {\"index\": 1, \"_id\": 0}).sort(\"index\", 1)\n",
    "        )\n",
    "        \n",
    "        # Raise an error if no data was found\n",
    "        if not self.doc_indexes:\n",
    "            raise ValueError(\"No documents found in the collection. Ensure each document has an 'index' field.\")\n",
    "\n",
    "        self.total_docs = len(self.doc_indexes)  # Store the number of total samples available\n",
    "\n",
    "        # Shuffle the list of indexes if requested\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.doc_indexes)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Returns the total number of samples in the dataset\n",
    "        return self.total_docs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # -------- Get document --------\n",
    "        unique_index = self.doc_indexes[idx][\"index\"]\n",
    "        doc = self.mongo_collection.find_one({\"index\": unique_index})\n",
    "        if doc is None:\n",
    "            raise KeyError(f\"Document with index {unique_index} not found.\")\n",
    "    \n",
    "        # -------- Load PSFs --------\n",
    "        nominal = np.array(doc[\"psf_nominal_array\"], dtype=np.float32)\n",
    "    \n",
    "        if not self.use_defocused:\n",
    "            raise ValueError(\n",
    "                \"U-Net image-to-image training requires 'use_defocus: True'.\"\n",
    "            )\n",
    "    \n",
    "        defocused = np.array(doc[\"psf_defocused_array\"], dtype=np.float32)\n",
    "    \n",
    "        # -------- Transform --------\n",
    "        if self.transform == \"sqrt\":\n",
    "            nominal = np.sqrt(nominal)\n",
    "            defocused = np.sqrt(defocused)\n",
    "        elif self.transform == \"log\":\n",
    "            nominal = np.log(nominal + 1e-6)\n",
    "            defocused = np.log(defocused + 1e-6)\n",
    "    \n",
    "        # -------- Input (nominal PSF) --------\n",
    "        input_tensor = torch.from_numpy(nominal).unsqueeze(0).float()  # (1,H,W)\n",
    "    \n",
    "        # -------- Target (defocused PSF) --------\n",
    "        target_tensor = torch.from_numpy(defocused).unsqueeze(0).float()  # (1,H,W)\n",
    "    \n",
    "        return input_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9412fa1-fa23-4413-aee2-ffeb3f3f120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Validate Config Values ---\n",
    "\n",
    "# Validate that 'seed' is present and is an integer\n",
    "if \"seed\" not in config[\"dataset\"]:\n",
    "    raise KeyError(\"'seed' must be defined in config['dataset'].\")\n",
    "if not isinstance(config[\"dataset\"][\"seed\"], int):\n",
    "    raise TypeError(f\"'seed' must be an integer, got {type(config['dataset']['seed'])}.\")\n",
    "\n",
    "# Validate that 'batch_size' is present and is an integer\n",
    "if \"batch_size\" not in config[\"training\"]:\n",
    "    raise KeyError(\"'batch_size' must be defined in config['training'].\")\n",
    "if not isinstance(config[\"training\"][\"batch_size\"], int):\n",
    "    raise TypeError(f\"'batch_size' must be an integer, got {type(config['training']['batch_size'])}.\")\n",
    "\n",
    "# --- Load the Full Dataset ---\n",
    "\n",
    "# Load the full dataset from a single collection\n",
    "full_dataset = DBImageDataset(\n",
    "    config=config,  # Pass the configuration containing all dataset-related settings\n",
    "    shuffle=False   # Disable shuffling here; shuffling will be handled at DataLoader level or after splitting\n",
    ")\n",
    "\n",
    "# --- Dataset Splitting ---\n",
    "\n",
    "# Define split ratios for the dataset\n",
    "train_ratio = 0.8  # 80% for training\n",
    "val_ratio = 0.1    # 10% for validation\n",
    "test_ratio = 0.1   # 10% for testing\n",
    "\n",
    "# Calculate total number of samples\n",
    "total_size = len(full_dataset)\n",
    "\n",
    "# Compute sizes for each split\n",
    "train_size = int(train_ratio * total_size)\n",
    "val_size = int(val_ratio * total_size)\n",
    "test_size = total_size - train_size - val_size  # Ensure all samples are used\n",
    "\n",
    "# Extract and use the seed from config\n",
    "split_seed = config[\"dataset\"][\"seed\"]\n",
    "\n",
    "# Perform reproducible random split using the validated seed\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    full_dataset,\n",
    "    [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(split_seed)\n",
    ")\n",
    "\n",
    "# --- Create DataLoaders ---\n",
    "\n",
    "# Extract validated batch size\n",
    "batch_size = config[\"training\"][\"batch_size\"]\n",
    "\n",
    "# Create DataLoader for each dataset split\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)   # Shuffle training data\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)      # No shuffle for validation\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)    # No shuffle for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1752317-011d-4c2d-90fd-005108bd2fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ResNetRegression(nn.Module):\n",
    "#     def __init__(self, config, num_outputs):\n",
    "#         super(ResNetRegression, self).__init__()\n",
    "#         self.cfg = config[\"model\"]\n",
    "#         self.use_phase_mag = config[\"dataset\"][\"use_phase_mag\"]  # Flag to use phase-magnitude ANN branch\n",
    "#         self.merge_strategy = self.cfg[\"add_magnitude_ann\"][\"merge_strategy\"] if self.use_phase_mag else None\n",
    "\n",
    "#         # --- Input Configuration ---\n",
    "#         zemax_sampling = config[\"zemax\"][\"sampling\"]\n",
    "#         self.image_size = 64 * 2**(zemax_sampling-1)  # Assumes 64x64 base PSF size scaled by sampling\n",
    "#         self.input_channels = 2 if config[\"dataset\"][\"use_defocus\"] else 1  # 1 for nominal, 2 if defocus is used\n",
    "#         self.output_dim = num_outputs  # Number of output regression parameters\n",
    "\n",
    "#         # --- Convolutional Backbone ---\n",
    "#         self.conv_layers = nn.ModuleList()\n",
    "#         in_channels = self.input_channels\n",
    "#         for layer_cfg in self.cfg[\"conv_layers\"]:\n",
    "#             out_channels = layer_cfg[\"out_channels\"]\n",
    "#             kernel_size = layer_cfg[\"kernel_size\"]\n",
    "#             padding = kernel_size // 2  # Keep spatial dimensions consistent\n",
    "#             activation = layer_cfg.get(\"activation\", True)  # Default to using ReLU\n",
    "\n",
    "#             block = [nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding, bias=False)]\n",
    "#             if activation:\n",
    "#                 block.append(nn.ReLU())\n",
    "#             if layer_cfg[\"pooling\"]:\n",
    "#                 block.append(nn.MaxPool2d(kernel_size=2, stride=2))  # Downsample spatial size by 2\n",
    "\n",
    "#             self.conv_layers.append(nn.Sequential(*block))\n",
    "#             in_channels = out_channels  # Update for next layer\n",
    "\n",
    "#         # --- Skip Connections ---\n",
    "#         self.skip_connections = self.cfg[\"skip_connections\"]\n",
    "#         self.skip_ops = nn.ModuleDict()\n",
    "#         for skip in self.skip_connections:\n",
    "#             from_layer = skip[\"from_layer\"]\n",
    "#             to_layer = skip[\"to_layer\"]\n",
    "#             skip_type = skip[\"type\"]\n",
    "#             in_ch = self.cfg[\"conv_layers\"][from_layer][\"out_channels\"]\n",
    "#             out_ch = self.cfg[\"conv_layers\"][to_layer][\"out_channels\"]\n",
    "\n",
    "#             if skip_type == \"conv+pool\":\n",
    "#                 # Align dimensions using downsampling and 1x1 convolution\n",
    "#                 self.skip_ops[f\"{from_layer}->{to_layer}\"] = nn.Sequential(\n",
    "#                     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#                     nn.Conv2d(in_ch, out_ch, kernel_size=1, bias=False)\n",
    "#                 )\n",
    "#             elif skip_type == \"add\":\n",
    "#                 self.skip_ops[f\"{from_layer}->{to_layer}\"] = nn.Identity()\n",
    "\n",
    "#         # --- Flatten Layer Output ---\n",
    "#         pool_count = sum(1 for l in self.cfg[\"conv_layers\"] if l[\"pooling\"])  # Count how many times we halved the size\n",
    "#         spatial_dim = self.image_size // (2 ** pool_count)  # Final spatial dimension after pooling\n",
    "#         final_out_channels = self.cfg[\"conv_layers\"][-1][\"out_channels\"]\n",
    "#         flatten_dim = final_out_channels * spatial_dim * spatial_dim\n",
    "\n",
    "#         # --- Fully Connected Layers for CNN Path ---\n",
    "#         self.fc_layers = nn.ModuleList()\n",
    "#         in_dim = flatten_dim\n",
    "#         for dim in self.cfg[\"fc_layers\"]:\n",
    "#             self.fc_layers.append(nn.Linear(in_dim, dim))\n",
    "#             in_dim = dim\n",
    "#         self.final_cnn_out_dim = in_dim  # Save final output size of CNN for merging\n",
    "\n",
    "#         # --- ANN Branch for Phase Magnitude Features ---\n",
    "#         if self.use_phase_mag:\n",
    "#             ann_hidden = self.cfg[\"add_magnitude_ann\"][\"hidden_layers\"]\n",
    "#             self.ann_layers = nn.ModuleList()\n",
    "            \n",
    "#             # Each PSF contributes row and column, so 2 * image_size; defocused adds another 2\n",
    "#             ann_in_dim = self.image_size * 2\n",
    "#             if config[\"dataset\"][\"use_defocus\"]:\n",
    "#                 ann_in_dim *= 2\n",
    "\n",
    "#             # Create fully connected layers for ANN\n",
    "#             for dim in ann_hidden:\n",
    "#                 self.ann_layers.append(nn.Linear(ann_in_dim, dim))\n",
    "#                 ann_in_dim = dim\n",
    "#             self.ann_out_dim = ann_in_dim  # Final ANN output dimension\n",
    "\n",
    "#             # Determine final dimension after merging CNN and ANN\n",
    "#             if self.merge_strategy == \"concat\":\n",
    "#                 merged_dim = self.final_cnn_out_dim + self.ann_out_dim\n",
    "#             elif self.merge_strategy == \"add\":\n",
    "#                 assert self.final_cnn_out_dim == self.ann_out_dim, \"Merge='add' requires ANN and CNN dims to match\"\n",
    "#                 merged_dim = self.final_cnn_out_dim\n",
    "\n",
    "#             self.fc_out = nn.Linear(merged_dim, self.output_dim)\n",
    "#         else:\n",
    "#             # If ANN is not used, project CNN output directly\n",
    "#             self.fc_out = nn.Linear(self.final_cnn_out_dim, self.output_dim)\n",
    "\n",
    "#     def forward(self, *inputs):\n",
    "#         # Support for optional second input (magnitude features)\n",
    "#         if self.use_phase_mag:\n",
    "#             x, magnitude_tensors = inputs\n",
    "#         else:\n",
    "#             x = inputs[0]\n",
    "\n",
    "#         # --- Forward Pass through Conv Layers with Skip Connections ---\n",
    "#         outputs = []\n",
    "#         for i, layer in enumerate(self.conv_layers):\n",
    "#             x = layer(x)\n",
    "#             outputs.append(x)\n",
    "\n",
    "#             # Apply skip connection if defined for this layer\n",
    "#             for skip in self.skip_connections:\n",
    "#                 if skip[\"to_layer\"] == i:\n",
    "#                     from_layer = skip[\"from_layer\"]\n",
    "#                     key = f\"{from_layer}->{i}\"\n",
    "#                     skip_out = self.skip_ops[key](outputs[from_layer])\n",
    "#                     x = x + skip_out  # Residual addition\n",
    "#                     outputs[i] = x\n",
    "\n",
    "#         x = F.relu(x)\n",
    "#         x = x.view(x.size(0), -1)  # Flatten tensor for FC layers\n",
    "\n",
    "#         # Pass through CNN's FC layers\n",
    "#         for fc in self.fc_layers:\n",
    "#             x = F.relu(fc(x))\n",
    "\n",
    "#         # --- Optional ANN Forward Pass and Merge ---\n",
    "#         if self.use_phase_mag:\n",
    "#             ann = magnitude_tensors.view(magnitude_tensors.size(0), -1)\n",
    "#             for layer in self.ann_layers:\n",
    "#                 ann = F.relu(layer(ann))\n",
    "\n",
    "#             if self.merge_strategy == \"concat\":\n",
    "#                 x = torch.cat([x, ann], dim=1)\n",
    "#             elif self.merge_strategy == \"add\":\n",
    "#                 x = x + ann\n",
    "\n",
    "#         # Final regression output\n",
    "#         return self.fc_out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cf6bd7e-700b-4a83-897a-15f03e7ecd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Utility blocks\n",
    "# =========================\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Conv -> BN -> ReLU -> Conv -> BN -> ReLU\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, kernel_size=3, dropout=0.0):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size, padding=padding, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size, padding=padding, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Dropout2d(dropout) if dropout > 0 else nn.Identity()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, pool=True, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.conv = DoubleConv(in_ch, out_ch, dropout=dropout)\n",
    "        self.pool = nn.MaxPool2d(2) if pool else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        skip = x\n",
    "        if self.pool:\n",
    "            x = self.pool(x)\n",
    "        return x, skip\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(in_ch, out_ch, dropout=dropout)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "\n",
    "        # Handle odd-sized inputs safely\n",
    "        if x.shape[-2:] != skip.shape[-2:]:\n",
    "            x = F.interpolate(x, size=skip.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# U-Net Model\n",
    "# =========================\n",
    "\n",
    "class UNetModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Config-driven U-Net for phase retrieval\n",
    "    Output: (B, 1, H, W)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        model_cfg = config[\"model\"]\n",
    "\n",
    "        enc_cfg = model_cfg[\"encoder\"]\n",
    "        dec_cfg = model_cfg[\"decoder\"]\n",
    "        bottleneck_cfg = model_cfg[\"bottleneck\"]\n",
    "\n",
    "        in_channels = model_cfg.get(\"input_channels\", 1)\n",
    "        out_channels = model_cfg[\"output\"][\"channels\"]\n",
    "        dropout = model_cfg.get(\"dropout\", 0.0)\n",
    "\n",
    "        # ---------- Encoder ----------\n",
    "        self.encoders = nn.ModuleList()\n",
    "        ch = in_channels\n",
    "\n",
    "        for layer in enc_cfg:\n",
    "            self.encoders.append(\n",
    "                EncoderBlock(\n",
    "                    ch,\n",
    "                    layer[\"out_channels\"],\n",
    "                    pool=layer.get(\"pooling\", True),\n",
    "                    dropout=dropout\n",
    "                )\n",
    "            )\n",
    "            ch = layer[\"out_channels\"]\n",
    "\n",
    "        # ---------- Bottleneck ----------\n",
    "        self.bottleneck = DoubleConv(\n",
    "            ch,\n",
    "            bottleneck_cfg[\"out_channels\"],\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        # ---------- Decoder ----------\n",
    "        self.decoders = nn.ModuleList()\n",
    "        ch = bottleneck_cfg[\"out_channels\"]\n",
    "\n",
    "        for layer in dec_cfg:\n",
    "            self.decoders.append(\n",
    "                DecoderBlock(\n",
    "                    ch,\n",
    "                    layer[\"out_channels\"],\n",
    "                    dropout=dropout\n",
    "                )\n",
    "            )\n",
    "            ch = layer[\"out_channels\"]\n",
    "\n",
    "        # ---------- Output ----------\n",
    "        self.final_conv = nn.Conv2d(ch, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skips = []\n",
    "\n",
    "        # Encoder\n",
    "        for enc in self.encoders:\n",
    "            x, skip = enc(x)\n",
    "            skips.append(skip)\n",
    "\n",
    "        # Bottleneck\n",
    "        x = self.bottleneck(x)\n",
    "\n",
    "        # Decoder\n",
    "        for dec, skip in zip(self.decoders, reversed(skips)):\n",
    "            x = dec(x, skip)\n",
    "\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61c089c4-fba6-4f53-a5ed-b5179c0689d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomRMSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        # predictions: Tensor of shape (B, N), where B = batch size, N = number of outputs\n",
    "        # targets:     Tensor of shape (B, N)\n",
    "\n",
    "        # Compute MSE per sample (along the output dimension)\n",
    "        row_mse = torch.mean((predictions - targets) ** 2, dim=1)  # Shape: (B,)\n",
    "\n",
    "        # Take sqrt to get RMSE per sample\n",
    "        row_rmse = torch.sqrt(row_mse)  # Shape: (B,)\n",
    "\n",
    "        # Average over batch to get final scalar loss\n",
    "        return torch.mean(row_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b0fe859-e15b-4262-ac1d-488b705352a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(train_losses, val_losses, test_loss, num_epochs, config):\n",
    "    # Extract model name and base path from config\n",
    "    model_name = config[\"model\"][\"name\"]\n",
    "    model_base_path = config[\"model\"][\"base_path\"]\n",
    "    \n",
    "    # Construct the full path where the plot will be saved\n",
    "    save_path = os.path.join(model_base_path, f\"loss_curve_{model_name}.png\")\n",
    "    \n",
    "    # Define epoch indices for the x-axis\n",
    "    epochs = range(1, num_epochs + 1)\n",
    "\n",
    "    # Create a new figure for the loss curves\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Plot training loss (solid line with circles)\n",
    "    plt.plot(epochs, train_losses, label=\"Training Loss\", marker='o', linestyle='-')\n",
    "    \n",
    "    # Plot validation loss (dashed line with squares)\n",
    "    plt.plot(epochs, val_losses, label=\"Validation Loss\", marker='s', linestyle='--')\n",
    "    \n",
    "    # Plot a horizontal line for the test loss\n",
    "    plt.axhline(y=test_loss, color='r', linestyle='-.', label=f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    # Set axis labels and plot title\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Training, Validation & Test Loss — {model_name}\")\n",
    "    \n",
    "    # Show legend and grid for better readability\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Make sure the output directory exists before saving the plot\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    \n",
    "    # Save the plot to the specified file path\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"Training plot saved to: {save_path}\")\n",
    "\n",
    "    # Display the plot inline\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2a9320e-cc9e-4a22-8fcb-7929cbf4b3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_loader, val_loader, test_loader,\n",
    "                       num_epochs, optimizer, criterion, device, config):\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_model_wts = None\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    avg_losses = []\n",
    "\n",
    "    model_name = config[\"model\"][\"name\"]\n",
    "    model_base_path = config[\"model\"][\"base_path\"]\n",
    "    os.makedirs(model_base_path, exist_ok=True)\n",
    "\n",
    "    # =========================\n",
    "    # Training + Validation\n",
    "    # =========================\n",
    "    for epoch in range(num_epochs):\n",
    "        # -------- Training --------\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_bar = tqdm(\n",
    "            train_loader,\n",
    "            desc=f\"Epoch {epoch+1}/{num_epochs} Training\",\n",
    "            leave=True\n",
    "        )\n",
    "\n",
    "        for batch in train_bar:\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            train_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "        epoch_train_loss = train_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "\n",
    "        # -------- Validation --------\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_bar = tqdm(\n",
    "            val_loader,\n",
    "            desc=f\"Epoch {epoch+1}/{num_epochs} Validation\",\n",
    "            leave=True\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_bar:\n",
    "                inputs, targets = batch\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "        epoch_val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "\n",
    "        epoch_avg_loss = (epoch_train_loss + epoch_val_loss) / 2\n",
    "        avg_losses.append(epoch_avg_loss)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs} Summary:\")\n",
    "        print(f\"Training Loss:   {epoch_train_loss:.4f}\")\n",
    "        print(f\"Validation Loss: {epoch_val_loss:.4f}\")\n",
    "        print(f\"Average Loss:    {epoch_avg_loss:.4f}\\n\")\n",
    "\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            best_model_wts = model.state_dict()\n",
    "            torch.save(\n",
    "                best_model_wts,\n",
    "                os.path.join(model_base_path, f\"{model_name}.pth\")\n",
    "            )\n",
    "            print(f\"Saved new best model (val loss = {best_val_loss:.4f})\")\n",
    "\n",
    "    # =========================\n",
    "    # Load best model\n",
    "    # =========================\n",
    "    model.load_state_dict(\n",
    "        torch.load(os.path.join(model_base_path, f\"{model_name}.pth\"))\n",
    "    )\n",
    "\n",
    "    # =========================\n",
    "    # Testing\n",
    "    # =========================\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_bar = tqdm(test_loader, desc=\"Testing Best Model\", leave=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_bar:\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            test_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    final_test_loss = test_loss / len(test_loader.dataset)\n",
    "    print(f\"\\nFinal Test Loss: {final_test_loss:.4f}\")\n",
    "\n",
    "    plot_training_history(train_losses, val_losses, final_test_loss,\n",
    "                          num_epochs, config)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae88e975-144d-43a0-b885-7ea9a83568ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set device to GPU if available, otherwise CPU\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Initialize model and move to device (GPU/CPU)\n",
    "# model = ResNetRegression(config=config, num_outputs=len(to_predict)).to(device)\n",
    "\n",
    "# # Initialize optimizer based on config\n",
    "# optimizer_name = config[\"training\"][\"optimizer\"]\n",
    "# optimizer = getattr(optim, optimizer_name)(\n",
    "#     model.parameters(), lr=config[\"training\"][\"learning_rate\"]\n",
    "# )\n",
    "\n",
    "# # Get number of epochs from config\n",
    "# num_epochs = config[\"training\"][\"num_epochs\"]\n",
    "\n",
    "# # Initialize loss function\n",
    "# criterion = CustomRMSELoss()\n",
    "\n",
    "# # Print model architecture\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77e25d09-0ca1-4317-a734-6c9038790def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNetModel(\n",
      "  (encoders): ModuleList(\n",
      "    (0): EncoderBlock(\n",
      "      (conv): DoubleConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "          (6): Dropout2d(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (1): EncoderBlock(\n",
      "      (conv): DoubleConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "          (6): Dropout2d(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (2): EncoderBlock(\n",
      "      (conv): DoubleConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "          (6): Dropout2d(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (bottleneck): DoubleConv(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Dropout2d(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (decoders): ModuleList(\n",
      "    (0): DecoderBlock(\n",
      "      (up): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (conv): DoubleConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "          (6): Dropout2d(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): DecoderBlock(\n",
      "      (up): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (conv): DoubleConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "          (6): Dropout2d(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): DecoderBlock(\n",
      "      (up): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (conv): DoubleConv(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "          (6): Dropout2d(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_conv): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Set device to GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize U-Net model and move to device\n",
    "model = UNetModel(config).to(device)\n",
    "\n",
    "# Initialize optimizer based on config\n",
    "optimizer_name = config[\"training\"][\"optimizer\"]\n",
    "optimizer = getattr(optim, optimizer_name)(\n",
    "    model.parameters(), lr=config[\"training\"][\"learning_rate\"]\n",
    ")\n",
    "\n",
    "# Get number of epochs from config\n",
    "num_epochs = config[\"training\"][\"num_epochs\"]\n",
    "\n",
    "# Initialize loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Print model architecture\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c067ac5-c3e6-4b63-8adc-aeecc07bbcf2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training U-Net model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 Training: 100%|██████████| 3/3 [00:01<00:00,  2.30it/s, loss=0.0638]\n",
      "Epoch 1/20 Validation: 100%|██████████| 1/1 [00:00<00:00, 11.77it/s, loss=0.0196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20 Summary:\n",
      "Training Loss:   0.1174\n",
      "Validation Loss: 0.0196\n",
      "Average Loss:    0.0685\n",
      "\n",
      "Saved new best model (val loss = 0.0196)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 Training: 100%|██████████| 3/3 [00:00<00:00,  5.08it/s, loss=0.0231]\n",
      "Epoch 2/20 Validation: 100%|██████████| 1/1 [00:00<00:00, 19.66it/s, loss=0.0164]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/20 Summary:\n",
      "Training Loss:   0.0359\n",
      "Validation Loss: 0.0164\n",
      "Average Loss:    0.0262\n",
      "\n",
      "Saved new best model (val loss = 0.0164)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 Training: 100%|██████████| 3/3 [00:00<00:00,  6.08it/s, loss=0.0261]\n",
      "Epoch 3/20 Validation: 100%|██████████| 1/1 [00:00<00:00, 20.83it/s, loss=0.0147]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/20 Summary:\n",
      "Training Loss:   0.0305\n",
      "Validation Loss: 0.0147\n",
      "Average Loss:    0.0226\n",
      "\n",
      "Saved new best model (val loss = 0.0147)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 Training: 100%|██████████| 3/3 [00:00<00:00,  5.67it/s, loss=0.0189]\n",
      "Epoch 4/20 Validation: 100%|██████████| 1/1 [00:00<00:00, 28.04it/s, loss=0.0141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/20 Summary:\n",
      "Training Loss:   0.0212\n",
      "Validation Loss: 0.0141\n",
      "Average Loss:    0.0177\n",
      "\n",
      "Saved new best model (val loss = 0.0141)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 Training: 100%|██████████| 3/3 [00:00<00:00,  5.95it/s, loss=0.0160]\n",
      "Epoch 5/20 Validation: 100%|██████████| 1/1 [00:00<00:00, 26.38it/s, loss=0.0141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/20 Summary:\n",
      "Training Loss:   0.0211\n",
      "Validation Loss: 0.0141\n",
      "Average Loss:    0.0176\n",
      "\n",
      "Saved new best model (val loss = 0.0141)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 Training: 100%|██████████| 3/3 [00:00<00:00,  6.03it/s, loss=0.0187]\n",
      "Epoch 6/20 Validation: 100%|██████████| 1/1 [00:00<00:00, 15.24it/s, loss=0.0142]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/20 Summary:\n",
      "Training Loss:   0.0152\n",
      "Validation Loss: 0.0142\n",
      "Average Loss:    0.0147\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 Training: 100%|██████████| 3/3 [00:00<00:00,  5.48it/s, loss=0.0204]\n",
      "Epoch 7/20 Validation: 100%|██████████| 1/1 [00:00<00:00, 25.40it/s, loss=0.0142]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/20 Summary:\n",
      "Training Loss:   0.0185\n",
      "Validation Loss: 0.0142\n",
      "Average Loss:    0.0164\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 Training: 100%|██████████| 3/3 [00:00<00:00,  6.33it/s, loss=0.0190]\n",
      "Epoch 8/20 Validation: 100%|██████████| 1/1 [00:00<00:00, 21.36it/s, loss=0.0141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/20 Summary:\n",
      "Training Loss:   0.0247\n",
      "Validation Loss: 0.0141\n",
      "Average Loss:    0.0194\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 Training: 100%|██████████| 3/3 [00:00<00:00,  5.99it/s, loss=0.0192]\n",
      "Epoch 9/20 Validation: 100%|██████████| 1/1 [00:00<00:00, 21.50it/s, loss=0.0140]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/20 Summary:\n",
      "Training Loss:   0.0172\n",
      "Validation Loss: 0.0140\n",
      "Average Loss:    0.0156\n",
      "\n",
      "Saved new best model (val loss = 0.0140)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 Training: 100%|██████████| 3/3 [00:00<00:00,  6.36it/s, loss=0.0134]\n",
      "Epoch 10/20 Validation: 100%|██████████| 1/1 [00:00<00:00, 21.59it/s, loss=0.0137]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/20 Summary:\n",
      "Training Loss:   0.0135\n",
      "Validation Loss: 0.0137\n",
      "Average Loss:    0.0136\n",
      "\n",
      "Saved new best model (val loss = 0.0137)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 Training: 100%|██████████| 3/3 [00:00<00:00,  6.10it/s, loss=0.0155]\n",
      "Epoch 11/20 Validation: 100%|██████████| 1/1 [00:00<00:00, 30.98it/s, loss=0.0133]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/20 Summary:\n",
      "Training Loss:   0.0204\n",
      "Validation Loss: 0.0133\n",
      "Average Loss:    0.0168\n",
      "\n",
      "Saved new best model (val loss = 0.0133)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 Training: 100%|██████████| 3/3 [00:00<00:00,  5.85it/s, loss=0.0359]\n",
      "Epoch 12/20 Validation: 100%|██████████| 1/1 [00:00<00:00, 18.24it/s, loss=0.0128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/20 Summary:\n",
      "Training Loss:   0.0226\n",
      "Validation Loss: 0.0128\n",
      "Average Loss:    0.0177\n",
      "\n",
      "Saved new best model (val loss = 0.0128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 Training: 100%|██████████| 3/3 [00:00<00:00,  5.92it/s, loss=0.0141]\n",
      "Epoch 13/20 Validation: 100%|██████████| 1/1 [00:00<00:00, 31.63it/s, loss=0.0124]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/20 Summary:\n",
      "Training Loss:   0.0156\n",
      "Validation Loss: 0.0124\n",
      "Average Loss:    0.0140\n",
      "\n",
      "Saved new best model (val loss = 0.0124)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 Training: 100%|██████████| 3/3 [00:00<00:00,  6.85it/s, loss=0.0122]\n",
      "Epoch 14/20 Validation: 100%|██████████| 1/1 [00:00<00:00, 30.68it/s, loss=0.0122]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/20 Summary:\n",
      "Training Loss:   0.0160\n",
      "Validation Loss: 0.0122\n",
      "Average Loss:    0.0141\n",
      "\n",
      "Saved new best model (val loss = 0.0122)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 Training: 100%|██████████| 3/3 [00:00<00:00,  6.63it/s, loss=0.0143]\n",
      "Epoch 15/20 Validation: 100%|██████████| 1/1 [00:00<00:00, 26.93it/s, loss=0.0120]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/20 Summary:\n",
      "Training Loss:   0.0175\n",
      "Validation Loss: 0.0120\n",
      "Average Loss:    0.0147\n",
      "\n",
      "Saved new best model (val loss = 0.0120)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 Training: 100%|██████████| 3/3 [00:00<00:00,  6.37it/s, loss=0.0125]\n",
      "Epoch 16/20 Validation: 100%|██████████| 1/1 [00:00<00:00, 23.67it/s, loss=0.0115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16/20 Summary:\n",
      "Training Loss:   0.0129\n",
      "Validation Loss: 0.0115\n",
      "Average Loss:    0.0122\n",
      "\n",
      "Saved new best model (val loss = 0.0115)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 Training: 100%|██████████| 3/3 [00:00<00:00,  6.00it/s, loss=0.0172]\n",
      "Epoch 17/20 Validation: 100%|██████████| 1/1 [00:00<00:00, 28.17it/s, loss=0.0108]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/20 Summary:\n",
      "Training Loss:   0.0183\n",
      "Validation Loss: 0.0108\n",
      "Average Loss:    0.0146\n",
      "\n",
      "Saved new best model (val loss = 0.0108)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 Training: 100%|██████████| 3/3 [00:00<00:00,  6.31it/s, loss=0.0162]\n",
      "Epoch 18/20 Validation: 100%|██████████| 1/1 [00:00<00:00, 22.11it/s, loss=0.0104]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18/20 Summary:\n",
      "Training Loss:   0.0126\n",
      "Validation Loss: 0.0104\n",
      "Average Loss:    0.0115\n",
      "\n",
      "Saved new best model (val loss = 0.0104)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 Training: 100%|██████████| 3/3 [00:00<00:00,  6.38it/s, loss=0.0136]\n",
      "Epoch 19/20 Validation: 100%|██████████| 1/1 [00:00<00:00, 25.71it/s, loss=0.0099]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19/20 Summary:\n",
      "Training Loss:   0.0145\n",
      "Validation Loss: 0.0099\n",
      "Average Loss:    0.0122\n",
      "\n",
      "Saved new best model (val loss = 0.0099)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 Training: 100%|██████████| 3/3 [00:00<00:00,  5.75it/s, loss=0.0163]\n",
      "Epoch 20/20 Validation: 100%|██████████| 1/1 [00:00<00:00, 20.47it/s, loss=0.0096]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20/20 Summary:\n",
      "Training Loss:   0.0170\n",
      "Validation Loss: 0.0096\n",
      "Average Loss:    0.0133\n",
      "\n",
      "Saved new best model (val loss = 0.0096)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Best Model: 100%|██████████| 1/1 [00:00<00:00, 28.55it/s, loss=0.0054]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Loss: 0.0054\n"
     ]
    }
   ],
   "source": [
    "# Get number of epochs from config\n",
    "num_epochs = config[\"training\"][\"num_epochs\"]\n",
    "\n",
    "# Print message indicating training start\n",
    "\n",
    "print(\"\\nTraining U-Net model...\")\n",
    "\n",
    "# Train and evaluate the model\n",
    "# Calls the train_and_evaluate function with model, data loaders, optimizer, and loss function\n",
    "model = train_and_evaluate(model, train_loader, val_loader, test_loader, num_epochs, optimizer, criterion, device, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bfffde-0beb-4be9-a772-f070628ebac9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
